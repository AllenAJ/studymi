# Home

> Build, test, and optimize GenAI apps from prototype to production. Comprehensive tracing, evaluation, and prompt optimization for RAG, agents, and more.

Opik is an [open-source](https://github.com/comet-ml/opik) logging, debugging, and optimization
platform for AI agents and LLM applications. If you're building AI features, you know it's easy to
spin up a working prototype but harder to log, test, iterate, and monitor to meet production
requirements.

Opik gives you all the tools you need to go from LLM observability to action across your AI
application footprint and dev cycle. Ship measurable improvements with gorgeous logs, annotation and
scoring functions, pre-configured LLM-as-a-judge [eval metrics](/evaluation/metrics/overview), and
even [automated agent optimization algorithms](/agent_optimization/overview) to maximize performance.

## End-to-End AI Engineering

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/home/EndToEnd-Engineering-Diagram.jpg" />
</Frame>

<Tip>
  Opik is Open Source! You can find the full source code on [GitHub](https://github.com/comet-ml/opik) and the complete
  self-hosting guide can be found [here](/self-host/local_deployment).
</Tip>

## Core Functions

<CardGroup cols={2}>
  <Card title="Quickstart Guide" href="/quickstart" icon="fa-solid fa-rocket" iconPosition="left">
    Opik integrates with your existing AI stack through your model provider or LLM framework.
  </Card>

  <Card title="LLM Observability - Log LLM Traces" href="/tracing/log_traces" icon="fa-solid fa-eye" iconPosition="left">
    Traces give you instant visibility into what's working, what's not, and why and includes
    advanced analysis and debugging features built in.
  </Card>

  <Card title="Evaluation - Score Performance" href="/evaluation/overview" icon="fa-solid fa-chart-line" iconPosition="left">
    Use LLM-as-a-judge and heuristic eval metrics to score your app or agent on hallucination,
    context recall, and more.
  </Card>

  <Card title="Agent Optimization" href="/agent_optimization/overview" icon="fa-solid fa-brain" iconPosition="left">
    Choose from six advanced optimization algorithms to auto-generate and score the best prompts for
    the steps in your agentic system.
  </Card>

  <Card title="Prompt Engineering" href="/prompt_engineering/prompt_management" icon="fa-solid fa-wand-magic-sparkles" iconPosition="left">
    Store and version system prompts, compare results live in the [Prompt Playground](/prompt_engineering/playground),
    and experiment with different models with our LLM proxy.
  </Card>

  <Card title="Self-hosting Opik" href="/self-host/overview" icon="fa-solid fa-server" iconPosition="left">
    Deploy Opik on your own infrastructure with local or Kubernetes deployment options.
  </Card>
</CardGroup>

## Video Tutorials

Prefer a visual guide ? Follow along as we cover everything from basic setup and trace logging to
LLM evaluation metrics, production monitoring, and more.

<Frame>
  <iframe width="100%" height="500px" src="https://www.youtube-nocookie.com/embed/TO9ar6-OJj4?rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen />
</Frame>

<Tip>
  You can find a full set of video tutorials in the [Opik University](/opik-university/overview).
</Tip>

## Open-Source access meets enterprise performance

All Opik versions ([cloud](https://www.comet.com/signup?from=llm),
[open source](https://github.com/comet-ml/opik), and
[enterprise](https://www.comet.com/site/pricing/)) include the full AI engineering featureset
and run on the Comet platform, with proven performance at scale supporting many of the world's
largest organizations.

Compare Opik to other LLM observability tools and you'll find that traces populate faster,
evaluations run smoother, and reliability comes standard â€” even for complex agentic systems serving
millions of users in production.

## Join Our Bounty Program!

Want to contribute to Opik and get rewarded for your efforts? Check out our
[Bounty Program](/contributing/developer-programs/bounties) to find exciting tasks and help us
grow the platform!


# Quickstart

> Integrate Opik with your LLM application to log calls and chains efficiently. Get started with our step-by-step guide.

This guide helps you integrate the Opik platform with your existing LLM application. The goal of
this guide is to help you log your first LLM calls and chains to the Opik platform.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/home/traces_page_for_quickstart.png" />
</Frame>

## Prerequisites

Before you begin, you'll need to choose how you want to use Opik:

* **Opik Cloud**: Create a free account at [comet.com/opik](https://www.comet.com/signup?from=llm\&utm_source=opik\&utm_medium=colab\&utm_content=quickstart\&utm_campaign=opik)
* **Self-hosting**: Follow the [self-hosting guide](/self-host/overview) to deploy Opik locally or on Kubernetes

## Logging your first LLM calls

Opik makes it easy to integrate with your existing LLM application, here are some of our most
popular integrations:

<Tabs>
  <Tab title="Python SDK" value="python-function-decorator">
    If you are using the Python function decorator, you can integrate by:

    <Steps>
      <Step>
        Install the Opik Python SDK:

        ```bash
        pip install opik
        ```
      </Step>

      <Step>
        Configure the Opik Python SDK:

        ```bash
        opik configure
        ```
      </Step>

      <Step>
        Wrap your function with the `@track` decorator:

        ```python
        from opik import track

        @track
        def my_function(input: str) -> str:
            return input
        ```

        All calls to the `my_function` will now be logged to Opik. This works well for any function
        even nested ones and is also supported by most integrations (just wrap any parent function
        with the `@track` decorator).
      </Step>
    </Steps>
  </Tab>

  <Tab title="TypeScript SDK" value="typescript-sdk">
    If you want to use the TypeScript SDK to log traces directly:

    <Steps>
      <Step>
        Install the Opik TypeScript SDK:

        ```bash
        npm install opik
        ```
      </Step>

      <Step>
        Configure the Opik TypeScript SDK by running the interactive CLI tool:

        ```bash
        npx opik-ts configure
        ```

        This will detect your project setup, install required dependencies, and help you configure environment variables.
      </Step>

      <Step>
        Log a trace using the Opik client:

        ```typescript
        import { Opik } from "opik";

        const client = new Opik();

        const trace = client.trace({
          name: "My LLM Application",
          input: { prompt: "What is the capital of France?" },
          output: { response: "The capital of France is Paris." },
        });

        trace.end();
        await client.flush();
        ```

        All traces will now be logged to Opik. You can also log spans within traces for more detailed observability.
      </Step>
    </Steps>
  </Tab>

  <Tab title="OpenAI (Python)" value="openai-python-sdk">
    If you are using the OpenAI Python SDK, you can integrate by:

    <Steps>
      <Step>
        Install the Opik Python SDK:

        ```bash
        pip install opik
        ```
      </Step>

      <Step>
        Configure the Opik Python SDK, this will prompt you for your API key if you are using Opik
        Cloud or your Opik server address if you are self-hosting:

        ```bash
        opik configure
        ```
      </Step>

      <Step>
        Wrap your OpenAI client with the `track_openai` function:

        ```python
        from opik.integrations.openai import track_openai
        from openai import OpenAI

        # Wrap your OpenAI client
        client = OpenAI()
        client = track_openai(client)

        # Use the client as normal
        completion = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "user", "content": "Hello, how are you?",
                },
            ],
        )
        print(completion.choices[0].message.content)
        ```

        All OpenAI calls made using the `client` will now be logged to Opik. You can combine
        this with the `@track` decorator to log the traces for each step of your agent.
      </Step>
    </Steps>
  </Tab>

  <Tab title="OpenAI (TS)" value="openai-ts-sdk">
    If you are using the OpenAI TypeScript SDK, you can integrate by:

    <Steps>
      <Step>
        Install the Opik TypeScript SDK:

        ```bash
        npm install opik-openai
        ```
      </Step>

      <Step>
        Configure the Opik TypeScript SDK by running the interactive CLI tool:

        ```bash
        npx opik-ts configure
        ```

        This will detect your project setup, install required dependencies, and help you configure environment variables.
      </Step>

      <Step>
        Wrap your OpenAI client with the `trackOpenAI` function:

        ```typescript
        import OpenAI from "openai";
        import { trackOpenAI } from "opik-openai";

        // Initialize the original OpenAI client
        const openai = new OpenAI({
          apiKey: process.env.OPENAI_API_KEY,
        });

        // Wrap the client with Opik tracking
        const trackedOpenAI = trackOpenAI(openai);

        // Use the tracked client just like the original
        const completion = await trackedOpenAI.chat.completions.create({
          model: "gpt-4",
          messages: [{ role: "user", content: "Hello, how can you help me today?" }],
        });
        console.log(completion.choices[0].message.content);

        // Ensure all traces are sent before your app terminates
        await trackedOpenAI.flush();
        ```

        All OpenAI calls made using the `trackedOpenAI` will now be logged to Opik.
      </Step>
    </Steps>
  </Tab>

  <Tab title="AI Vercel SDK" value="ai-vercel-sdk">
    If you are using the AI Vercel SDK, you can integrate by:

    <Steps>
      <Step>
        Install the Opik Vercel integration:

        ```bash
        npm install opik-vercel
        ```
      </Step>

      <Step>
        Configure the Opik AI Vercel SDK by running the interactive CLI tool:

        ```bash
        npx opik-ts configure
        ```

        This will detect your project setup, install required dependencies, and help you configure environment variables.
      </Step>

      <Step>
        Initialize the OpikExporter with your AI SDK:

        ```ts
        import { openai } from "@ai-sdk/openai";
        import { generateText } from "ai";
        import { NodeSDK } from "@opentelemetry/sdk-node";
        import { getNodeAutoInstrumentations } from "@opentelemetry/auto-instrumentations-node";
        import { OpikExporter } from "opik-vercel";

        // Set up OpenTelemetry with Opik
        const sdk = new NodeSDK({
          traceExporter: new OpikExporter(),
          instrumentations: [getNodeAutoInstrumentations()],
        });
        sdk.start();

        // Your AI SDK calls with telemetry enabled
        const result = await generateText({
          model: openai("gpt-4o"),
          prompt: "What is love?",
          experimental_telemetry: { isEnabled: true },
        });

        console.log(result.text);
        ```

        All AI SDK calls with `experimental_telemetry: { isEnabled: true }` will now be logged to Opik.
      </Step>
    </Steps>
  </Tab>

  <Tab title="Ollama" value="ollama-python">
    If you are using Ollama with Python, you can integrate by:

    <Steps>
      <Step>
        Install the Opik Python SDK:

        ```bash
        pip install opik
        ```
      </Step>

      <Step>
        Configure the Opik Python SDK:

        ```bash
        opik configure
        ```
      </Step>

      <Step>
        Integrate Opik with your Ollama calls:

        <Tabs>
          <Tab title="Ollama Python Package">
            Wrap your Ollama calls with the `@track` decorator:

            ```python
            import ollama
            from opik import track

            @track
            def ollama_call(user_message: str):
                response = ollama.chat(
                    model='llama3.1',
                    messages=[{'role': 'user', 'content': user_message}]
                )
                return response['message']

            # Call your function
            result = ollama_call("Say this is a test")
            print(result)
            ```
          </Tab>

          <Tab title="OpenAI SDK">
            Use Opik's OpenAI integration with Ollama's OpenAI-compatible API:

            ```python
            from openai import OpenAI
            from opik.integrations.openai import track_openai

            # Create an OpenAI client pointing to Ollama
            client = OpenAI(
                base_url='http://localhost:11434/v1/',
                api_key='ollama'  # required but ignored
            )

            # Wrap the client with Opik tracking
            client = track_openai(client)

            # Call the local Ollama model
            response = client.chat.completions.create(
                model='llama3.1',
                messages=[{'role': 'user', 'content': 'Say this is a test'}]
            )
            print(response.choices[0].message.content)
            ```
          </Tab>

          <Tab title="LangChain">
            Use Opik's LangChain integration with Ollama:

            ```python
            from langchain_ollama import ChatOllama
            from opik.integrations.langchain import OpikTracer

            # Create the Opik tracer
            opik_tracer = OpikTracer()

            # Create the Ollama model with Opik tracing
            llm = ChatOllama(
                model="llama3.1",
                temperature=0,
            ).with_config({"callbacks": [opik_tracer]})

            # Call the Ollama model
            messages = [
                ("system", "You are a helpful assistant."),
                ("human", "Say this is a test")
            ]
            response = llm.invoke(messages)
            print(response)
            ```
          </Tab>
        </Tabs>

        All Ollama calls will now be logged to Opik. See the [full Ollama guide](/integrations/ollama) for more advanced usage.
      </Step>
    </Steps>
  </Tab>

  <Tab title="ADK" value="adk-python">
    If you are using the ADK, you can integrate by:

    <Steps>
      <Step>
        Install the Opik SDK:

        ```bash
        pip install opik google-adk
        ```
      </Step>

      <Step>
        Configure the Opik SDK by running the `opik configure` command in your terminal:

        ```bash
        opik configure
        ```
      </Step>

      <Step>
        Wrap your ADK agent with the `OpikTracer`:

        ```python
        from google.adk.agents import Agent
        from opik.integrations.adk import OpikTracer, track_adk_agent_recursive

        # Create your ADK agent
        agent = Agent(
            name="helpful_assistant",
            model="gemini-2.0-flash",
            instruction="You are a helpful assistant that answers user questions."
        )

        # Wrap your ADK agent with the OpikTracer
        opik_tracer = OpikTracer()
        track_adk_agent_recursive(agent, opik_tracer)
        ```

        All ADK agent calls will now be logged to Opik.
      </Step>
    </Steps>
  </Tab>

  <Tab title="LangGraph" value="langgraph">
    If you are using LangGraph, you can integrate by:

    <Steps>
      <Step>
        Install the Opik SDK:

        ```bash
        pip install opik
        ```
      </Step>

      <Step>
        Configure the Opik SDK by running the `opik configure` command in your terminal:

        ```bash
        opik configure
        ```
      </Step>

      <Step>
        Track your LangGraph graph with `track_langgraph`:

        ```python
        from opik.integrations.langchain import OpikTracer, track_langgraph

        # Create your LangGraph graph
        graph = ...
        app = graph.compile(...)

        # Create OpikTracer and track the graph once
        # The graph visualization is automatically extracted by track_langgraph
        opik_tracer = OpikTracer()
        app = track_langgraph(app, opik_tracer)

        # Now all invocations are automatically tracked!
        result = app.invoke({"messages": [HumanMessage(content = "How to use LangGraph ?")]})
        ```

        All LangGraph calls will now be logged to Opik. No need to pass callbacks on every invocation!
      </Step>
    </Steps>
  </Tab>

  <Tab title="AI Wizard" value="ai-installation">
    <div>
      <span>
        <p>
          Integrate with Opik faster using this pre-built prompt
        </p>
      </span>

      <Button intent="primary" href="cursor:////anysphere.cursor-deeplink/prompt?text=%23+OPIK+Agentic+Onboarding%0A%0A%23%23+Goals%0A%0AYou+must+help+me%3A%0A%0A1.+Integrate+the+Opik+client+with+my+existing+LLM+application%0A2.+Set+up+tracing+for+my+LLM+calls+and+chains%0A%0A%23%23+Rules%0A%0ABefore+you+begin%2C+you+must+understand+and+strictly+adhere+to+these+core+principles%3A%0A%0A1.+Code+Preservation+%26+Integration+Guidelines%3A%0A%0A+++-+Existing+business+logic+must+remain+untouched+and+unmodified%0A+++-+Only+add+Opik-specific+code+%28decorators%2C+imports%2C+handlers%2C+env+vars%29%0A+++-+Integration+must+be+non-invasive+and+backwards+compatible%0A%0A2.+Process+Requirements%3A%0A%0A+++-+Follow+the+workflow+steps+sequentially+without+deviation%0A+++-+Validate+completion+of+each+step+before+proceeding%0A+++-+Request+explicit+approval+for+any+workflow+modifications%0A%0A3.+Documentation+%26+Resources%3A%0A%0A+++-+Reference+official+Opik+documentation+at+https%3A%2F%2Fwww.comet.com%2Fdocs%2Fopik%2Fquickstart.md%0A+++-+Follow+Opik+best+practices+and+recommended+patterns%0A+++-+Maintain+detailed+integration+notes+and+configuration+details%0A%0A4.+Testing+%26+Validation%3A%0A+++-+Verify+Opik+integration+without+impacting+existing+functionality%0A+++-+Validate+tracing+works+correctly+for+all+LLM+interactions%0A+++-+Ensure+proper+error+handling+and+logging%0A%0A%23%23+Integration+Workflow%0A%0A%23%23%23+Step+1%3A+Language+and+Compatibility+Check%0A%0AFirst%2C+analyze+the+codebase+to+identify%3A%0A%0A1.+Primary+programming+language+and+frameworks%0A2.+Existing+LLM+integrations+and+patterns%0A%0ACompatibility+Requirements%3A%0A%0A-+Supported+Languages%3A+Python%2C+JavaScript%2FTypeScript%0A%0AIf+the+codebase+uses+unsupported+languages%3A%0A%0A-+Stop+immediately%0A-+Inform+me+that+the+codebase+is+unsupported+for+AI+integration%0A%0AOnly+proceed+to+Step+2+if%3A%0A%0A-+Language+is+Python+or+JavaScript%2FTypeScript%0A%0A%23%23%23+Step+2%3A+Codebase+Discovery+%26+Entrypoint+Confirmation%0A%0AAfter+verifying+language+compatibility%2C+perform+a+full+codebase+scan+with+the+following+objectives%3A%0A%0A-+LLM+Touchpoints%3A+Locate+all+files+and+functions+that+invoke+or+interface+with+LLMs+or+can+be+a+candidates+for+tracing.%0A-+Entrypoint+Detection%3A+Identify+the+primary+application+entry+point%28s%29+%28e.g.%2C+main+script%2C+API+route%2C+CLI+handler%29.+If+ambiguous%2C+pause+and+request+clarification+on+which+component%28s%29+are+most+important+to+trace+before+proceeding.%0A++%E2%9A%A0%EF%B8%8F+Do+not+proceed+to+Step+3+without+explicit+confirmation+if+the+entrypoint+is+unclear.%0A-+Return+the+LLM+Touchpoints+to+me%0A%0A%23%23%23+Step+3%3A+Discover+Available+Integrations%0A%0AAfter+I+confirm+the+LLM+Touchpoints+and+entry+point%2C+find+the+list+of+supported+integrations+at+https%3A%2F%2Fwww.comet.com%2Fdocs%2Fopik%2Fintegrations%2Foverview.md%0A%0A%23%23%23+Step+4%3A+Deep+Analysis+Confirmed+files+for+LLM+Frameworks+%26+SDKs%0A%0AUsing+the+files+confirmed+in+Step+2%2C+perform+targeted+inspection+to+detect+specific+LLM-related+technologies+in+use%2C+such+as%3A%0ASDKs%3A+openai%2C+anthropic%2C+huggingface%2C+etc.%0AFrameworks%3A+LangChain%2C+LlamaIndex%2C+Haystack%2C+etc.%0A%0A%23%23%23+Step+5%3A+Pre-Implementation+Development+Plan+%28Approval+Required%29%0A%0ADo+not+write+or+modify+code+yet.+You+must+propose+me+a+step-by-step+plan+including%3A%0A%0A-+Opik+packages+to+install%0A-+Files+to+be+modified%0A-+Code+snippets+for+insertion%2C+clearly+scoped+and+annotated%0A-+Where+to+place+Opik+API+keys%2C+with+placeholder+comments+%28Visit+https%3A%2F%2Fcomet.com%2Fopik%2Fyour-workspace-name%2Fget-started+to+copy+your+API+key%29%0A++Wait+for+approval+before+proceeding%21%0A%0A%23%23%23+Step+6%3A+Execute+the+Integration+Plan%0A%0AAfter+approval%3A%0A%0A-+Run+the+package+installation+command+via+terminal+%28pip+install+opik%2C+npm+install+opik%2C+etc.%29.%0A-+Apply+code+modifications+exactly+as+described+in+Step+5.%0A-+Keep+all+additions+minimal+and+non-invasive.%0A++Upon+completion%2C+review+the+changes+made+and+confirm+installation+success.%0A%0A%23%23%23+Step+7%3A+Request+User+Review+and+Wait%0A%0ANotify+me+that+all+integration+steps+are+complete.%0A%22Please+run+the+application+and+verify+if+Opik+is+capturing+traces+as+expected.+Let+me+know+if+you+need+adjustments.%22%0A%0A%23%23%23+Step+8%3A+Debugging+Loop+%28If+Needed%29%0A%0AIf+issues+are+reported%3A%0A%0A1.+Parse+the+error+or+unexpected+behavior+from+feedback.%0A2.+Re-query+the+Opik+docs+using+https%3A%2F%2Fwww.comet.com%2Fdocs%2Fopik%2Fquickstart.md+if+needed.%0A3.+Propose+a+minimal+fix+and+await+approval.%0A4.+Apply+and+revalidate.%0A">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" id="Ebene_1" version="1.1" viewBox="0 0 466.73 532.09">
            <path class="st0" d="M457.43,125.94L244.42,2.96c-6.84-3.95-15.28-3.95-22.12,0L9.3,125.94c-5.75,3.32-9.3,9.46-9.3,16.11v247.99c0,6.65,3.55,12.79,9.3,16.11l213.01,122.98c6.84,3.95,15.28,3.95,22.12,0l213.01-122.98c5.75-3.32,9.3-9.46,9.3-16.11v-247.99c0-6.65-3.55-12.79-9.3-16.11h-.01ZM444.05,151.99l-205.63,356.16c-1.39,2.4-5.06,1.42-5.06-1.36v-233.21c0-4.66-2.49-8.97-6.53-11.31L24.87,145.67c-2.4-1.39-1.42-5.06,1.36-5.06h411.26c5.84,0,9.49,6.33,6.57,11.39h-.01Z" />
          </svg>

          Open in Cursor
        </div>
      </Button>
    </div>

    The pre-built prompt will guide you through the integration process, install the Opik SDK and
    instrument your code. It supports both Python and TypeScript codebases, if you are using
    another language just let us know and we can help you out.

    Once the integration is complete, simply run your application and you will start seeing traces
    in your Opik dashboard.
  </Tab>

  <Tab title="All integrations" value="all_integrations">
    Opik has **30+ integrations** with popular frameworks and model providers:

    <CardGroup cols={3}>
      <Card title="LangChain" href="/integrations/langchain" icon={<img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/tracing/langchain.svg" />} iconPosition="left" />

      <Card title="LlamaIndex" href="/integrations/llama_index" icon={<img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/tracing/llamaindex.svg" />} iconPosition="left" />

      <Card title="Anthropic" href="/integrations/anthropic" icon={<img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/tracing/anthropic.svg" />} iconPosition="left" />

      <Card title="AWS Bedrock" href="/integrations/bedrock" icon={<img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/tracing/bedrock.svg" />} iconPosition="left" />

      <Card title="Google Gemini" href="/integrations/gemini" icon={<img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/tracing/gemini.svg" />} iconPosition="left" />

      <Card title="CrewAI" href="/integrations/crewai" icon={<img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/tracing/crewai.svg" />} iconPosition="left" />
    </CardGroup>

    **[View all 30+ integrations â†’](/integrations/overview)**
  </Tab>
</Tabs>

## Analyze your traces

After running your application, you will start seeing your traces in your Opik dashboard:

<video src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/tracing/quickstart.mp4" width="854" height="480" autoPlay muted loop playsInline preload="auto" />

If you don't see traces appearing, reach out to us on [Slack](https://chat.comet.com) or raise an issue on [GitHub](https://github.com/comet-ml/opik/issues) and we'll help you troubleshoot.

## Next steps

Now that you have logged your first LLM calls and chains to Opik, why not check out:

1. [In depth guide on agent observability](/tracing/log_traces): Learn how to customize the data
   that is logged to Opik and how to log conversations.
2. [Opik Experiments](/evaluation/concepts): Opik allows you to automated the evaluation process of
   your LLM application so that you no longer need to manually review every LLM response.
3. [Opik's evaluation metrics](/evaluation/metrics/overview): Opik provides a suite of evaluation
   metrics (Hallucination, Answer Relevance, Context Recall, etc.) that you can use to score your
   LLM responses.


# Quickstart notebook

> Learn to automate LLM workflow evaluation and track calls with Opik using Chain of Density Summarization for Arxiv papers.

# Quickstart notebook - Summarization

In this notebook, we will look at how you can use Opik to track your LLM calls, chains and agents. We will introduce the concept of tracing and how to automate the evaluation of your LLM workflows.

We will be using a technique called Chain of Density Summarization to summarize Arxiv papers. You can learn more about this technique in the [From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting](https://arxiv.org/abs/2309.04269) paper.

## Getting started

We will first install the required dependencies and configure both Opik and OpenAI.

```python
%pip install -U opik openai requests PyPDF2 --quiet
```

[Comet](https://www.comet.com/site?from=llm\&utm_source=opik\&utm_medium=colab\&utm_content=langchain\&utm_campaign=opik) provides a hosted version of the Opik platform, [simply create an account](https://www.comet.com/signup?from=llm\&utm_source=opik\&utm_medium=colab\&utm_content=langchain\&utm_campaign=opik) and grab your API Key.

> You can also run the Opik platform locally, see the [installation guide](https://www.comet.com/docs/opik/self-host/overview/?from=llm\&utm_source=opik\&utm_medium=colab\&utm_content=langchain\&utm_campaign=opik) for more information.

```python
import opik

# Configure Opik
opik.configure()
```

## Implementing Chain of Density Summarization

The idea behind this approach is to first generate a sparse candidate summary and then iteratively refine it with missing information without making it longer. We will start by defining two prompts:

1. Iteration summary prompt: This prompt is used to generate and refine a candidate summary.
2. Final summary prompt: This prompt is used to generate the final summary from the sparse set of candidate summaries.

```python
import opik

ITERATION_SUMMARY_PROMPT = opik.Prompt(
    name="Iteration Summary Prompt",
    prompt="""
Document: {{document}}
Current summary: {{current_summary}}
Instruction to focus on: {{instruction}}

Generate a concise, entity-dense, and highly technical summary from the provided Document that specifically addresses the given Instruction.

Guidelines:
- Make every word count: If there is a current summary re-write it to improve flow, density and conciseness.
- Remove uninformative phrases like "the article discusses".
- The summary should become highly dense and concise yet self-contained, e.g. , easily understood without the Document.
- Make sure that the summary specifically addresses the given Instruction
""".rstrip().lstrip(),
)

FINAL_SUMMARY_PROMPT = opik.Prompt(
    name="Final Summary Prompt",
    prompt="""
Given this summary: {{current_summary}}
And this instruction to focus on: {{instruction}}
Create an extremely dense, final summary that captures all key technical information in the most concise form possible, while specifically addressing the given instruction.
""".rstrip().lstrip(),
)
```

We can now define the summarization chain by combining the two prompts. In order to track the LLM calls, we will use Opik's integration with OpenAI through the `track_openai` function and we will add the `@opik.track` decorator to each function so we can track the full chain and not just individual LLM calls:

```python
from opik.integrations.openai import track_openai
from openai import OpenAI
import opik

# Use a dedicated quickstart endpoint, replace with your own OpenAI API Key in your own code
openai_client = track_openai(
    OpenAI(
        base_url="https://odbrly0rrk.execute-api.us-east-1.amazonaws.com/Prod/",
        api_key="Opik-Quickstart",
    )
)


@opik.track
def summarize_current_summary(
    document: str,
    instruction: str,
    current_summary: str,
    model: str = "gpt-4o-mini",
):
    prompt = ITERATION_SUMMARY_PROMPT.format(
        document=document, current_summary=current_summary, instruction=instruction
    )

    response = openai_client.chat.completions.create(
        model=model, max_tokens=4096, messages=[{"role": "user", "content": prompt}]
    )

    return response.choices[0].message.content


@opik.track
def iterative_density_summarization(
    document: str,
    instruction: str,
    density_iterations: int,
    model: str = "gpt-4o-mini",
):
    summary = ""
    for iteration in range(1, density_iterations + 1):
        summary = summarize_current_summary(document, instruction, summary, model)
    return summary


@opik.track
def final_summary(instruction: str, current_summary: str, model: str = "gpt-4o-mini"):
    prompt = FINAL_SUMMARY_PROMPT.format(
        current_summary=current_summary, instruction=instruction
    )

    return (
        openai_client.chat.completions.create(
            model=model, max_tokens=4096, messages=[{"role": "user", "content": prompt}]
        )
        .choices[0]
        .message.content
    )


@opik.track(project_name="Chain of Density Summarization")
def chain_of_density_summarization(
    document: str,
    instruction: str,
    model: str = "gpt-4o-mini",
    density_iterations: int = 2,
):
    summary = iterative_density_summarization(
        document, instruction, density_iterations, model
    )
    final_summary_text = final_summary(instruction, summary, model)

    return final_summary_text
```

Let's call the summarization chain with a sample document:

```python
import textwrap

document = """
Artificial intelligence (AI) is transforming industries, revolutionizing healthcare, finance, education, and even creative fields. AI systems
today are capable of performing tasks that previously required human intelligence, such as language processing, visual perception, and
decision-making. In healthcare, AI assists in diagnosing diseases, predicting patient outcomes, and even developing personalized treatment plans.
In finance, it helps in fraud detection, algorithmic trading, and risk management. Education systems leverage AI for personalized learning, adaptive
testing, and educational content generation. Despite these advancements, ethical concerns such as data privacy, bias, and the impact of AI on employment
remain. The future of AI holds immense potential, but also significant challenges.
"""

instruction = "Summarize the main contributions of AI to different industries, and highlight both its potential and associated challenges."

summary = chain_of_density_summarization(document, instruction)

print("\n".join(textwrap.wrap(summary, width=80)))
```

Thanks to the `@opik.track` decorator and Opik's integration with OpenAI, we can now track the entire chain and all the LLM calls in the Opik UI:

![Trace UI](https://raw.githubusercontent.com/comet-ml/opik/main/apps/opik-documentation/documentation/static/img/cookbook/chain_density_trace_cookbook.png)

## Automatting the evaluation process

### Defining a dataset

Now that we have a working chain, we can automate the evaluation process. We will start by defining a dataset of documents and instructions:

```python
import opik

dataset_items = [
    {
        "pdf_url": "https://arxiv.org/pdf/2301.00234",
        "title": "A Survey on In-context Learning",
        "instruction": "Summarize the key findings on the impact of prompt engineering in in-context learning.",
    },
    {
        "pdf_url": "https://arxiv.org/pdf/2301.03728",
        "title": "Scaling Laws for Generative Mixed-Modal Language Models",
        "instruction": "How do scaling laws apply to generative mixed-modal models according to the paper?",
    },
    {
        "pdf_url": "https://arxiv.org/pdf/2308.10792",
        "title": "Instruction Tuning for Large Language Models: A Survey",
        "instruction": "What are the major challenges in instruction tuning for large language models identified in the paper?",
    },
    {
        "pdf_url": "https://arxiv.org/pdf/2302.08575",
        "title": "Foundation Models in Natural Language Processing: A Survey",
        "instruction": "Explain the role of foundation models in the current natural language processing landscape.",
    },
    {
        "pdf_url": "https://arxiv.org/pdf/2306.13398",
        "title": "Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey",
        "instruction": "What are the cutting edge techniques used in multi-modal pre-training models?",
    },
    {
        "pdf_url": "https://arxiv.org/pdf/2103.07492",
        "title": "Continual Learning in Neural Networks: An Empirical Evaluation",
        "instruction": "What are the main challenges of continual learning for neural networks according to the paper?",
    },
    {
        "pdf_url": "https://arxiv.org/pdf/2304.00685v2",
        "title": "Vision-Language Models for Vision Tasks: A Survey",
        "instruction": "What are the most widely used vision-language models?",
    },
    {
        "pdf_url": "https://arxiv.org/pdf/2303.08774",
        "title": "GPT-4 Technical Report",
        "instruction": "What are the main differences between GPT-4 and GPT-3.5?",
    },
    {
        "pdf_url": "https://arxiv.org/pdf/2406.04744",
        "title": "CRAG -- Comprehensive RAG Benchmark",
        "instruction": "What was the approach to experimenting with different data mixtures?",
    },
]

client = opik.Opik()
DATASET_NAME = "arXiv Papers"
dataset = client.get_or_create_dataset(name=DATASET_NAME)
dataset.insert(dataset_items)
```

*Note:* Opik automatically deduplicates dataset items to make it easier to iterate on your dataset.

### Defining the evaluation metrics

Opik includes a [library of evaluation metrics](https://www.comet.com/docs/opik/evaluation/metrics/overview) that you can use to evaluate your chains. For this particular example, we will be using a custom metric that evaluates the relevance, conciseness and technical accuracy of each summary

```python
from opik.evaluation.metrics import base_metric, score_result
import json

# We will define the response format so the output has the correct schema. You can also use structured outputs with Pydantic models for this.
json_schema = {
    "type": "json_schema",
    "json_schema": {
        "name": "summary_evaluation_schema",
        "schema": {
            "type": "object",
            "properties": {
                "relevance": {
                    "type": "object",
                    "properties": {
                        "score": {
                            "type": "integer",
                            "minimum": 1,
                            "maximum": 5,
                            "description": "Score between 1-5 for how well the summary addresses the instruction",
                        },
                        "explanation": {
                            "type": "string",
                            "description": "Brief explanation of the relevance score",
                        },
                    },
                    "required": ["score", "explanation"],
                },
                "conciseness": {
                    "type": "object",
                    "properties": {
                        "score": {
                            "type": "integer",
                            "minimum": 1,
                            "maximum": 5,
                            "description": "Score between 1-5 for how concise the summary is while retaining key information",
                        },
                        "explanation": {
                            "type": "string",
                            "description": "Brief explanation of the conciseness score",
                        },
                    },
                    "required": ["score", "explanation"],
                },
                "technical_accuracy": {
                    "type": "object",
                    "properties": {
                        "score": {
                            "type": "integer",
                            "minimum": 1,
                            "maximum": 5,
                            "description": "Score between 1-5 for how accurately the summary conveys technical details",
                        },
                        "explanation": {
                            "type": "string",
                            "description": "Brief explanation of the technical accuracy score",
                        },
                    },
                    "required": ["score", "explanation"],
                },
            },
            "required": ["relevance", "conciseness", "technical_accuracy"],
            "additionalProperties": False,
        },
    },
}


# Custom Metric: One template/prompt to extract 4 scores/results
class EvaluateSummary(base_metric.BaseMetric):
    # Constructor
    def __init__(self, name: str):
        self.name = name

    def score(
        self, summary: str, instruction: str, model: str = "gpt-4o-mini", **kwargs
    ):
        prompt = f"""
            Summary: {summary}
            Instruction: {instruction}

            Evaluate the summary based on the following criteria:
            1. Relevance (1-5): How well does the summary address the given instruction?
            2. Conciseness (1-5): How concise is the summary while retaining key information?
            3. Technical Accuracy (1-5): How accurately does the summary convey technical details?

            Your response MUST be in the following JSON format:
            {{
                "relevance": {{
                    "score": <int>,
                    "explanation": "<string>"
                }},
            "conciseness": {{
                "score": <int>,
                "explanation": "<string>"
                }},
            "technical_accuracy": {{
                "score": <int>,
                "explanation": "<string>"
                }}
            }}

            Ensure that the scores are integers between 1 and 5, and that the explanations are concise.
        """

        response = openai_client.chat.completions.create(
            model=model,
            max_tokens=1000,
            messages=[{"role": "user", "content": prompt}],
            response_format=json_schema,
        )

        eval_dict = json.loads(response.choices[0].message.content)

        return [
            score_result.ScoreResult(
                name="summary_relevance",
                value=eval_dict["relevance"]["score"],
                reason=eval_dict["relevance"]["explanation"],
            ),
            score_result.ScoreResult(
                name="summary_conciseness",
                value=eval_dict["conciseness"]["score"],
                reason=eval_dict["conciseness"]["explanation"],
            ),
            score_result.ScoreResult(
                name="summary_technical_accuracy",
                value=eval_dict["technical_accuracy"]["score"],
                reason=eval_dict["technical_accuracy"]["explanation"],
            ),
            score_result.ScoreResult(
                name="summary_average_score",
                value=round(sum(eval_dict[k]["score"] for k in eval_dict) / 3, 2),
                reason="The average of the 3 summary evaluation metrics",
            ),
        ]
```

### Create the task we want to evaluate

We can now create the task we want to evaluate. In this case, we will have the dataset item as an input and return a dictionary containing the summary and the instruction so that we can use this in the evaluation metrics:

```python
import requests
import io
from PyPDF2 import PdfReader
from typing import Dict


# Load and extract text from PDFs
@opik.track
def load_pdf(pdf_url: str) -> str:
    # Download the PDF
    response = requests.get(pdf_url)
    pdf_file = io.BytesIO(response.content)

    # Read the PDF
    pdf_reader = PdfReader(pdf_file)

    # Extract text from all pages
    text = ""
    for page in pdf_reader.pages:
        text += page.extract_text()

    # Truncate the text to 100000 characters as this is the maximum supported by OpenAI
    text = text[:100000]
    return text


def evaluation_task(x: Dict):
    text = load_pdf(x["pdf_url"])
    instruction = x["instruction"]
    model = MODEL
    density_iterations = DENSITY_ITERATIONS

    result = chain_of_density_summarization(
        document=text,
        instruction=instruction,
        model=model,
        density_iterations=density_iterations,
    )

    return {"summary": result}
```

### Run the automated evaluation

We can now use the `evaluate` method to evaluate the summaries in our dataset:

```python
from opik.evaluation import evaluate

MODEL = "gpt-4o-mini"
DENSITY_ITERATIONS = 2

experiment_config = {
    "model": MODEL,
    "density_iterations": DENSITY_ITERATIONS,
}

res = evaluate(
    dataset=dataset,
    experiment_config=experiment_config,
    task=evaluation_task,
    scoring_metrics=[EvaluateSummary(name="summary-metrics")],
    prompts=[ITERATION_SUMMARY_PROMPT, FINAL_SUMMARY_PROMPT],
    project_name="Chain of Density Summarization - Experiments",
)
```

The experiment results are now available in the Opik UI:

![Trace UI](https://raw.githubusercontent.com/comet-ml/opik/main/apps/opik-documentation/documentation/static/img/cookbook/chain_density_experiment_cookbook.png)

## Comparing prompt templates

We will update the iteration summary prompt and evaluate its impact on the evaluation metrics.

```python
import opik

ITERATION_SUMMARY_PROMPT = opik.Prompt(
    name="Iteration Summary Prompt",
    prompt="""Document: {{document}}
Current summary: {{current_summary}}
Instruction to focus on: {{instruction}}

Generate a concise, entity-dense, and highly technical summary from the provided Document that specifically addresses the given Instruction.

Guidelines:
1. **Maximize Clarity and Density**: Revise the current summary to enhance flow, density, and conciseness.
2. **Eliminate Redundant Language**: Avoid uninformative phrases such as "the article discusses."
3. **Ensure Self-Containment**: The summary should be dense and concise, easily understandable without referring back to the document.
4. **Align with Instruction**: Make sure the summary specifically addresses the given instruction.

""".rstrip().lstrip(),
)
```

```python
from opik.evaluation import evaluate

MODEL = "gpt-4o-mini"
DENSITY_ITERATIONS = 2

experiment_config = {
    "model": MODEL,
    "density_iterations": DENSITY_ITERATIONS,
}

res = evaluate(
    dataset=dataset,
    experiment_config=experiment_config,
    task=evaluation_task,
    scoring_metrics=[EvaluateSummary(name="summary-metrics")],
    prompts=[ITERATION_SUMMARY_PROMPT, FINAL_SUMMARY_PROMPT],
    project_name="Chain of Density Summarization - Experiments",
)
```

You can now compare the results between the two experiments in the Opik UI:

![Trace UI](https://raw.githubusercontent.com/comet-ml/opik/main/apps/opik-documentation/documentation/static/img/cookbook/chain_density_trace_comparison_cookbook.png)


# Roadmap

> Explore Opik's evolving roadmap and contribute ideas to enhance our open-source project as we develop new features and improve functionality.

Opik is [Open-Source](https://github.com/comet-ml/opik) and is under very active development. We use the feedback from the Opik community to drive the roadmap, this is very much a living document that will change as we release new features and learn about new ways to improve the product.

<Tip>
  If you have any ideas or suggestions for the roadmap, you can create a [new Feature Request issue](https://github.com/comet-ml/opik/issues/new/choose) in the Opik Github repo.
</Tip>

## ðŸš€ What have we recently launched?

Curious about our latest updates? Explore everything weâ€™ve shipped in our [changelog](/docs/opik/changelog).

## ðŸ› ï¸ What are we currently working on?

We are currently working on both improving existing features and developing new features:

**Agents & Optimization**:

* New capabilities for agent and prompt optimization

**Strategic work**:

* Advanced multi-modal support
* New Webhook Alerts System

**Developer Experience**:

* JS/TS SDK and REST API improved support

**User Experience**:

* Improved onboarding experience
* Updated docs and mobile experience

## ðŸ§­ What is planned next?

**Agents & Optimization**:

* Simplified optimization workflow: the new Optimization Studio.
* Build better AIs: advanced agent optimization powered by new algorithms.
* Multi-objective optimizer with support for latency and cost

**User Experience**:

* Enhanced table support across the product: smarter sorting, flexible filtering, seamless exporting, and powerful bulk actions for effortless data management.

## ðŸ”­ What's coming later?

Looking a bit further ahead, we plan to invest in deeper optimization, collaboration and observability tooling:

**Advanced evaluation**:

* Smarter AI performance assessment: Reinforcement Learning from Human Feedback for LLM-as-a-Judge metrics.

**Closing the improvement loop**:

* Automatically optimize agents across the entire lifecycle by leveraging production data and key performance metrics

**Projects & Dataset improvements**:

* Dataset editing and versioning
* Support public and private projects
* Introduce evaluation features in the Opik playground

**Monitoring & Analytics**:

* Introduce dashboarding features

## ðŸ’¬ Provide your feedback

We rely on your feedback to shape the roadmap and decide which features to prioritize next. You can upvote existing ideas or even
add your own on [Github Issues](https://github.com/comet-ml/opik/issues/).

*Last updated: Q4 2025*


# FAQ

> Explore common questions about Opik, learn how to get started, and find assistance to enhance your experience with our powerful tool.

These FAQs are a collection of the most common questions that we've received from our users. If you
have any questions or need additional assistance, please open an
[issue on GitHub](https://github.com/comet-ml/opik/issues).

## General

### What is Opik's relationship with Comet?

Opik is developed by Comet and is available in multiple deployment options:

* As an open-source standalone product that can be used locally or self-hosted on your own
  infrastructure
* As an integrated part of the Comet MLOps Platform (both in Comet-hosted and self-hosted
  deployments)

This means you can use Opik independently of the Comet platform, or as part of your existing Comet
MLOps setup.

### What SDKs does Opik provide?

Opik currently provides official SDKs for:

* Python: Our most feature-complete SDK, supporting all Opik features
* TypeScript: Rich tracing capabilities

These SDKs are actively maintained and regularly updated. For other languages, you can use our REST
API directly - see our [API documentation](/docs/opik/reference/rest-api/overview) for details.

### What format should I use for start\_time?

The `start_time` field supports **ISO 8601 datetime format** with UTC timezone. For best
compatibility across all Opik SDKs and the backend, use:

```
2024-01-01T10:20:30.123456Z
```

**Format specification:**

* Pattern: `YYYY-MM-DDTHH:MM:SS.ffffffZ`
* Timezone: UTC (always with `Z` suffix)
* Precision: Microseconds (6 decimal places)

**Examples:**

* `2024-01-01T10:20:30Z` (seconds only)
* `2024-01-01T10:20:30.123Z` (milliseconds)
* `2024-01-01T10:20:30.123456Z` (microseconds - recommended)

This format is supported by:

* Python SDK
* TypeScript SDK
* Java Backend
* Frontend UI
* ClickHouse Database

**Why UTC with Z suffix?**

* Avoids timezone conversion issues
* Universally supported across programming languages
* Explicitly indicates UTC timezone
* RFC 3339 compliant

### Can I use Opik to monitor my LLM application in production?

Yes, Opik has been designed from the ground up to be used to monitor production applications. If you
are self-hosting the
Opik platform, we recommend using the [Kubernetes deployment](/self-host/overview) option to ensure
that Opik can scale as needed.

### What is the difference between Opik Cloud and the Open-Source Opik platform ?

The Opik Cloud platform is the hosted version of Opik that is available to both
free users and paying customers. It includes all the features you love about
the open-source version of Opik, plus user management, billing and support without
the hassle of setting up and maintaining your own Opik platform.

The Open-Source version of the Opik product includes tracing and online evaluation
features so you can monitor your LLMs in production. It also includes advanced
evaluation features including evaluation metrics and an advanced experiment
comparison UI. Less technical users can also use Opik to review production traces
or run experiments from the Opik Playground.

## Opik Cloud

### Where can I find my Opik API key ?

The Opik API key is needed to log data to either the Opik Cloud platform. You can
find your API key in either your Account settings or in the user menu available
from the top right of the page

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/faq/opik_api_key.png" />
</Frame>

<Tip>
  If you are using the Open-Source Opik platform, you will not have Opik API keys. You can configure Opik by running
  `opik configure` in your terminal which will prompt you for your Opik deployment and create all the required
  configurations.
</Tip>

### How do I find my workspace and project name?

Your **workspace name** and **project name** are displayed in the Opik UI:

**Workspace Name:**

* Look at the top of the page in the breadcrumb navigation
* It appears as the first item after "opik" in the breadcrumb path
* Example: `opik > your-workspace-name > Projects > your-project-name`

**Project Name:**

* When you're inside a project, it appears as the main header in the content area
* It's also the last item in the breadcrumb navigation
* Example: `opik > your-workspace-name > Projects > your-project-name`

You can also see your workspace information in the left sidebar under "Projects" which shows the
count of projects in your workspace.

### Are there are rate limits on Opik Cloud?

Yes, in order to ensure all users have a good experience we have implemented rate limits. When you
encounter a rate limit, endpoints will return the status code `429`.

There's a global rate limit of `2,000` request/minute per user across all REST API endpoints, with
an extra burst of `100` requests.

Afterward, there's a data ingestion limit of `10,000` events/minute per user. An event is any trace,
span, feedback score, dataset item, experiment item, etc. which is ingested, stored and persisted by
Opik.

Additionally, there's another data ingestion limit of `5,000` events/minute per workspace and per
user.

Finally, there's a rate limit of `250` requests/minute per user for the `Get span by id` endpoint:
`GET /api/v1/private/spans/:id`.

<Note>
  The Python SDK has implemented some logic to slow down the logging to avoid data loss when
  encountering rate limits. You will see the message: `OPIK: Ingestion rate limited, retrying in 55 seconds, remaining queue size: 1, ...`.

  If you are using other logging methods, you will need to implement your own "backoff and retry"
  strategy
</Note>

For questions about rate limits, reach out to us on [Slack](https://chat.comet.com).

## Integrations

### What integrations does Opik support?

Opik supports a comprehensive range of popular LLM frameworks, providers, and tools. You can find
detailed integration guides in our [Integrations Overview](/integrations/overview).

**Model Providers:**
Anthropic, AWS Bedrock, BytePlus, Cloudflare Workers AI, Cohere, DeepSeek, Fireworks AI, Google
Gemini, Groq, Mistral AI, Novita AI, Ollama, OpenAI (Python & JS/TS), Predibase, Together AI, IBM
WatsonX, xAI Grok

**Frameworks:**
AG2, Agno, Autogen, CrewAI, DSPy, Haystack, Instructor, LangChain (Python & JS/TS), LangGraph,
LlamaIndex, Mastra, Pydantic AI, Semantic Kernel, Smolagents, Spring AI, Strands Agents, VoltAgent,
OpenAI Agents, Google Agent Development Kit, LiveKit Agents, BeeAI

**Evaluation & Testing:**
Ragas

**Gateways & Proxies:**
LiteLLM, OpenRouter, AISuite

**No-Code Tools:**
Dify, Flowise

**OpenTelemetry:**
OpenTelemetry (Python & Ruby SDKs)

**Other Tools:**
Guardrails AI

### What if Opik doesn't support my preferred framework or tool?

If you don't see your preferred framework or tool listed in our integrations, we encourage you to:

1. Open an [issue on GitHub](https://github.com/comet-ml/opik/issues) to request the integration
2. In the meantime, you can manually log your LLM interactions using our SDK's core logging
   functions - see our [tracing documentation](/docs/opik/tracing/log_traces) for examples

We actively maintain and expand our integration support based on community feedback.

## Troubleshooting

### Why am I getting 403 errors?

If you're encountering 403 (Forbidden) errors, this typically indicates an authentication or
authorization issue. If you haven't configured your credentials yet, the easiest way to get started
is to run:

```bash
opik configure
```

This interactive command will guide you through setting up the required configuration.

Otherwise, please double-check your existing configuration:

For Opik Cloud by Comet:

* `api_key` (required): Verify your API key is correct and active
* `workspace` (required): Confirm you have access to the specified workspace
* `project_name` (optional): If specified, ensure the project name is valid
* `url_override`: Should be set to `https://www.comet.com/opik/api` (this is the default)

For Self-hosted Opik:

* `url_override` (required): Verify your base URL points to your Opik instance (e.g.,
  `http://your-instance:5173/api`)

You can find your current configuration in the Opik configuration file (`~/.opik.config`) or by
checking your environment variables (`OPIK_API_KEY`, `OPIK_WORKSPACE`, `OPIK_URL_OVERRIDE`,
`OPIK_PROJECT_NAME`). For more details on configuration, see our
[SDK Configuration guide](/docs/opik/tracing/sdk_configuration).

## How can I diagnose issues with Opik?

If you are experiencing any problems using Opik, such as receiving 400 or 500 errors from the
backend, or being unable to connect at all, we recommend running the following command in your
terminal:

```bash
opik healthcheck
```

This command will analyze your configuration and backend connectivity, providing useful insights
into potential issues.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/healthcheck.png" />
</Frame>

Reviewing these sections can help pinpoint the source of the problem and suggest possible
resolutions.

### âŒ¨ï¸ Using Comet Debugger Mode (UI/Browser)

**Comet Debugger Mode** is a hidden diagnostic feature in the **Opik web application** that displays real-time technical information to help you troubleshoot issues. This mode is particularly useful when investigating connectivity problems, reporting bugs, or verifying your deployment version.

**To toggle Comet Debugger Mode:**

Press `Command + Shift + .` on macOS or `Ctrl + Shift + .` on Windows/Linux

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/faq/comet_debugger_mode.png" alt="Comet Debugger Mode showing RTT and version" />
</Frame>

**What it displays:**

* **Network Status**: Real-time connectivity indicator with RTT (Round Trip Time) showing latency to the Opik backend server in seconds
* **Opik Version**: The current version of Opik you're running (click to copy to clipboard)

This information is helpful when:

* Reporting issues to the Opik team (include the version number and RTT)
* Verifying your Opik version matches expected deployment
* Diagnosing connectivity problems between UI and backend (check RTT for latency issues)
* Troubleshooting UI-related issues or unexpected behavior
* Confirming successful updates or deployments
* Monitoring network performance and latency to the backend server

**How it works:**

The keyboard shortcut toggles the debug information overlay on and off. When enabled, a small
status bar appears in the UI showing the network connectivity status and version information.
The mode persists across browser sessions (stored in local storage), so you only need to enable
it once until you toggle it off again.

<Note>
  The debugger mode setting persists across sessions and is stored in your browser's local storage.
  Press the keyboard shortcut again to hide the debug information.
</Note>


# January 27, 2026

Here are the most relevant improvements we've made since the last release:

## ðŸš€ Optimization Studio

We're excited to introduce **Optimization Studio** â€” a powerful new way to improve your prompts without writing code. Bring a prompt, define what "good" looks like, and Opik tests variations to find a better version you can ship with confidence.

<video src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-01-27T08:27:40.904Z/img/agent_optimization/optimization_studio_walkthrough.mp4" width="854" height="480" autoPlay muted loop playsInline preload="auto" />

**What's new:**

* **No-code prompt optimization** - Optimization Studio helps you improve prompts directly from the Opik UI. You see scores and examples, not just a hunch, shortening the loop from idea to evidence
* **Algorithm selection** - Choose how Opik searches for better prompts: **GEPA** works well for single-turn prompts and quick improvements, while **HRPO** is better when you need deeper analysis of why a prompt fails
* **Flexible metrics** - Define how Opik should score each prompt variation. Use **Equals** for strict matching when you have a single correct answer, or **G-Eval** when answers can vary and you want a model to grade quality
* **Visual progress tracking** - Monitor your optimization runs with real-time progress charts showing the best score so far and results for each trial
* **Trials comparison** - The Trials tab lets you compare prompt variations and scores side-by-side, with the ability to drill down into individual evaluated items
* **Rerun & compare** - Easily rerun the same setup, cancel a run to change inputs, or select multiple runs to compare outcomes

For teams that prefer a programmatic workflow, we've also released **Opik Optimizer SDK v3** with improved algorithms, better performance, and more intuitive APIs.

ðŸ‘‰ [Optimization Studio Documentation](https://www.comet.com/docs/opik/agent_optimization/optimization_studio)

## ðŸ“Š Dashboard Improvements

We've enhanced the dashboard with new widgets and visualization capabilities to help you track and compare experiments more effectively.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2026-01-27/experiment-leaderboard.png" alt="Experiment Leaderboard widget showing ranked experiments with feedback scores and metrics charts" />
</Frame>

**What's new:**

* **Experiment Leaderboard Widget** - A new leaderboard widget lets you rank and compare experiments at a glance directly from your dashboard, making it easier to identify your best-performing configurations
* **Group By for Metrics Widget** - The project metrics widget now supports grouping, allowing you to slice and dice your metrics data in more meaningful ways
* **Span-level Metrics Charts** - New charts provide visibility into span-level metrics, giving you deeper insights into the performance of individual components in your traces

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2026-01-27/span-metrics-dashboard.png" alt="Project metrics dashboard showing span count, duration percentiles, and spans by model provider" />
</Frame>

## ðŸŽ¬ Video Generation Support

We've added support for the latest video generation models, enabling you to track and log video outputs from your AI applications.

**What's new:**

* **OpenAI SORA Integration** - Log and track video generation outputs from OpenAI's SORA model directly in Opik
* **Google Veo Integration** - Full support for Google's Veo video generation API, including automatic logging of video outputs and metadata

## ðŸ§ª Experiment Management

We've made it easier to organize and navigate your experiments with new filtering and tagging capabilities.

**What's improved:**

* **Project Column in Experiments View** - Experiments now display their associated project directly in the list view, making it easier to understand context at a glance
* **Project Filter & Grouping** - Filter and group your experiments by project to quickly find what you're looking for across large experiment collections
* **Experiment Tags** - Tags are now rendered on the experiment page, helping you categorize and identify experiments more easily

## âœ¨ UI/UX Improvements

We've made several improvements to make your day-to-day workflow smoother.

**What's improved:**

* **Time Formatting Settings** - Customize how timestamps are displayed throughout the UI to match your preferred format
* **Online Score Rules Defaults** - Input and output fields in online score rules are now pre-populated with sensible defaults, reducing setup time
* **Dataset Item Navigation** - Navigation tags in the experiment item view now link directly to the associated dataset item for easier data exploration
* **Annotation Queue Review** - You can now review completed annotation queues, making it easier to audit and verify your annotation work

## ðŸ”Œ SDK & Integrations

We've improved our SDK integrations with better tracing and performance metrics.

**What's improved:**

* **Vercel AI SDK Thread Support** - Thread ID support for the Vercel AI SDK integration enables better conversation tracking across multi-turn interactions
* **ADK Distributed Tracing** - Added distributed trace headers support to the ADK integration for improved observability in distributed systems
* **Time-to-First-Token (TTFT)** - The ADK integration now captures TTFT metrics, giving you visibility into response latency for streaming applications

***

And much more! ðŸ‘‰ [See full commit log on GitHub](https://github.com/comet-ml/opik/compare/1.9.78...1.9.101)

*Releases*: `1.9.79`, `1.9.80`, `1.9.81`, `1.9.82`, `1.9.83`, `1.9.84`, `1.9.85`, `1.9.86`, `1.9.87`, `1.9.88`, `1.9.89`, `1.9.90`, `1.9.91`, `1.9.92`, `1.9.95`, `1.9.96`, `1.9.97`, `1.9.98`, `1.9.99`, `1.9.100`, `1.9.101`


# January 13, 2026

Here are the most relevant improvements we've made since the last release:

## ðŸ”Œ Playground & Provider Enhancements

We've expanded the Playground with new provider support and enhanced functionality to make prompt experimentation more powerful.

**What's new:**

* **Display Metric Results in Output** - Playground output cells now display metric results directly, making it easier to evaluate prompt performance at a glance

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2026-01-13/Playground-metric-results.png" alt="Playground output showing metric results directly in the output cell" />
</Frame>

* **Model Selector for OpikAI Features** - Easily select which model powers the Prompt Generator and Prompt Improver features

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2026-01-13/OpikAI-model-selector.png" alt="Model selector dropdown for OpikAI features like Prompt Generator and Prompt Improver" />
</Frame>

* **Native AWS Bedrock Integration** - Bedrock is now available as a native provider in the Playground, giving you direct access to Amazon's models without additional configuration

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2026-01-13/AWS_Bedrock.png" alt="AWS Bedrock integration in the Playground provider selection" />
</Frame>

* **Gemini 3 Flash Support** - Added support for Gemini 3 Flash in both the Playground and online scoring, expanding your model options for fast, cost-effective evaluations

## ðŸ§ª Online Evaluation & Scoring

We've made online evaluation more flexible and easier to manage across your projects.

**What's improved:**

* **Multi-Project Evaluation Rules** - Online evaluation rules can now be applied across multiple projects, reducing duplication and simplifying rule management

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2026-01-13/Multi-project-rule-support.png" alt="Multi-project support for online evaluation rules" />
</Frame>

* **Clone Score Rules** - Quickly duplicate existing online score rules to create variations without starting from scratch

## ðŸŽ¨ UI & UX Enhancements

We've refined the user experience across the platform with improved responsiveness and dashboard polish.

**What's improved:**

* **Mobile Responsiveness** - Better support for mobile devices when logging traces
* **Dashboard Enhancements** - Unified widget editor design, dashboard count in sidebar, and various UX improvements to the dashboard experience

## ðŸ“¦ SDK Improvements

We've updated our SDKs with new capabilities and modernized dependencies.

**What's new:**

* **Python 3.9 End of Life** - Python 3.9 support has been retired as it reached end-of-life. Please upgrade to Python 3.10+
* **Experiment Tags in evaluate()** - You can now add tags to experiments directly when calling the `evaluate()` method
* **Vercel AI SDK v6** - Upgraded TypeScript SDK integration from Vercel AI SDK v5 to v6
* **Prompt Version Tags (TypeScript)** - TypeScript SDK now supports prompt version tags for better prompt management

***

And much more! ðŸ‘‰ [See full commit log on GitHub](https://github.com/comet-ml/opik/compare/1.9.56...1.9.78)

*Releases*: `1.9.57`, `1.9.58`, `1.9.59`, `1.9.60`, `1.9.61`, `1.9.62`, `1.9.63`, `1.9.64`, `1.9.65`, `1.9.66`, `1.9.67`, `1.9.68`, `1.9.69`, `1.9.70`, `1.9.71`, `1.9.72`, `1.9.73`, `1.9.74`, `1.9.75`, `1.9.76`, `1.9.77`, `1.9.78`


# December 18, 2025

Here are the most relevant improvements we've made since the last release:

## ðŸ“Š Custom Dashboards (Beta)

Custom Dashboards are now live! ðŸŽ‰

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/production/dashboard_example.png" alt="Custom dashboard example showing customizable widgets for tracking metrics across projects and experiments" />
</Frame>

Our new dashboards engine lets you build fully customizable views to track everything from token usage and cost to latency, quality across projects and experiments.

**ðŸ“ Where to find them?**

Dashboards are available in three places inside Opik:

* **Dashboards page** â€“ create and manage all dashboards from the sidebar
* **Project page** â€“ view project-specific metrics under the Dashboards tab
* **Experiment comparison page** â€“ visualize and compare experiment results

**ðŸ§© Built-in templates to get started fast**

We ship dashboards with zero-setup pre-built templates, including Performance Overview, Experiment Insights and Project Operational Metrics.

Templates are fully editable and can be saved as new dashboards once customized.

**ðŸ§± Flexible widgets**

Dashboards support multiple widget types:

* **Project Metrics** (time-series and bar charts for project data)
* **Project Statistics** (KPI number cards)
* **Experiment Metrics** (line, bar, radar charts for experiment data)
* **Markdown** (notes, documentation, context)

Widgets support filtering, grouping, resizing, drag-and-drop layouts, and global date range controls.

ðŸ‘‰ [Documentation](https://www.comet.com/docs/opik/production/dashboards)

## ðŸ§ª Improved Evaluation Capabilities

**Span-Level Metrics**

Span-level metrics are officially live in Opik supporting both LLMaaJ and code-based metrics!

Teams can now EASILY evaluate the quality of specific steps inside their agent flows with full precision.
Instead of assessing only the final output or top-level trace, you can attach metrics directly to individual call spans or segments of an agent's trajectory.

This unlocks dramatically finer-grained visibility and control. For example:

* Score critical decision points inside task-oriented or tool-using agents
* Measure the performance of sub-tasks independently to pinpoint bottlenecks or regressions
* Compare step-by-step agent behavior across runs, experiments, or versions

**New Support accessing full tree, subtree, or leaf nodes in Online Scores**

This update enhances the online scoring engine to support referencing entire root objects (input, output, metadata) in LLM-as-Judge and code-based evaluators, not just nested fields within them.

Online Scoring previously only exposed leaf-level values from an LLM's structured output. With this update, Opik now supports rendering any subtree: from individual nodes to entire nested structures.

## ðŸ“ Tags Support & Metadata Filtering for Prompt Version Management

You can now tag individual prompt versions (not just the prompt!).

This provides a clean, intuitive way to mark best-performing versions, manage lifecycles, and integrate version selection into agent deployments.

## ðŸŽ¥ More Multimodal Support: now Audio!

Now you can pass audio as part of your prompts, in the playground and on online evals for advanced multimodal scenarios.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-12-18/audio_playground.png" alt="Audio support in playground showing audio prompts and file upload options" />
</Frame>

## ðŸ“ˆ More Insights!

**Thread-level insights**

Added new metrics to the threads table with thread-level metrics and statistics, providing users with aggregated insights about their full multi-turn agentic interactions:

* Duration percentiles (p50, p90, p99) and averages
* Token usage statistics (total, prompt, completion tokens)
* Cost metrics and aggregations
* Also added filtering support by project, time range, and custom filters

**Experiment insights**

Added additional aggregation methods in headers for experiment items.

This new release adds percentile aggregation methods (p50, p90, p99) for all numerical metrics in experiment items table headers, extending the existing pattern used for duration to cost, feedback scores, and total tokens.

## ðŸ”Œ Integrations

**Support for GPT-5.2 in Playground and Online Scoring**

Added full support for GPT 5.2 models in both the playground and online scoring features for OpenAI and OpenRouter providers.

**Harbor Integration**

Added a comprehensive Opik integration for Harbor, a benchmark evaluation framework for autonomous LLM agents. The integration enables observability for agent benchmark evaluations (SWE-bench, LiveCodeBench, Terminal-Bench, etc.).

ðŸ‘‰ [Harbor Integration Documentation](https://www.comet.com/docs/opik/integrations/harbor)

***

And much more! ðŸ‘‰ [See full commit log on GitHub](https://github.com/comet-ml/opik/compare/1.9.40...1.9.56)

*Releases*: `1.9.41`, `1.9.42`, `1.9.43`, `1.9.44`, `1.9.45`, `1.9.46`, `1.9.47`, `1.9.48`, `1.9.49`, `1.9.50`, `1.9.51`, `1.9.52`, `1.9.53`, `1.9.54`, `1.9.55`, `1.9.56`


# December 9, 2025

Here are the most relevant improvements we've made since the last release:

## ðŸ“Š Dataset Improvements

We've enhanced dataset functionality with several key improvements:

* **Edit Dataset Items** - You can now edit dataset items directly from the UI, making it easier to update and refine your evaluation data.

* **Remove Dataset Upload Limit for Self-Hosted** - Self-hosted deployments no longer have dataset upload limits, giving you more flexibility for large-scale evaluations.

* **Dataset Item Tagging Support** - Added comprehensive tagging support for dataset items, enabling better organization and filtering of your evaluation data.

* **Dataset Filtering Capabilities by Any Column** - Filter datasets by any column in both the playground and dataset view, giving you flexible ways to find and work with specific data subsets.

* **Ability to Rename Datasets** - Rename datasets directly from the UI, making it easier to organize and manage your evaluation datasets.

## ðŸ“ˆ Experiment Updates

We've made significant improvements to experiment management and analysis:

* **Experiment-Level Metrics** - Compute experiment-level metrics (as opposed to experiment-item-level metrics) for better insights into your evaluation results. Read more in the [experiment-level metrics documentation](https://www.comet.com/docs/opik/evaluation/evaluate_your_llm#computing-experiment-level-metrics).

* **Rename Experiments & Metadata** - Update experiment names and metadata config directly from the dashboard, giving you more control over experiment organization.

* **Token & Cost Columns** - Token usage and cost are now surfaced in the experiment items table for easy scanning and cost visibility.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-12-09/experiment_updates.png" alt="Experiments page showing experiment-level metrics, graphs, and experiment items table with token and cost columns" />
</Frame>

## ðŸŽ® Playground Improvements

We've made the Playground more powerful and easier to use for non-technical users:

* **Easy Navigation from Playground to Dataset and Metrics** - Quick navigation links from the playground to related datasets and metrics, streamlining your workflow.

* **Advanced filtering for Playground Datasets** - Filter playground datasets by tags and any other columns, making it easier to find and work with specific dataset items.

* **Pagination for the Playground** - Added pagination support to handle large datasets more efficiently in the playground.

* **Added Experiment Progress Bar in the Playground** - Visual progress indicators for running experiments, giving you real-time feedback on experiment status.

* **Added Model-Specific Throttling and Concurrency Configs in the Playground** - Configure throttling and concurrency settings per model in the playground, giving you fine-grained control over resource usage.

## ðŸš¨ Enhanced Alerts

We've expanded alert capabilities with threshold support:

* **Added Threshold Support for Trace and Thread Feedback Scores** - Configure thresholds for feedback scores on traces and threads, enabling more precise alerting based on quality metrics.

* **Added Threshold to Trace Error Alerts** - Set thresholds for trace error alerts to get notified only when error rates exceed your configured limits.

* **Trigger Experiment Created Alert from the Playground** - Receive alerts when experiments are created directly from the playground.

## ðŸ¤– Opik Optimizer Updates

Significant enhancements to the Opik Optimizer:

* **Cost and Latency Optimization Support** - Added support for optimizing both cost and latency metrics simultaneously. Read more in the [optimization metrics documentation](/docs/opik/agent_optimization/optimization/define_metrics#include-cost-and-duration-metrics).

* **Training and Validation Dataset Support** - Introduced support for training and validation dataset splits, enabling better optimization workflows. Learn more in the [dataset documentation](/docs/opik/agent_optimization/optimization/define_datasets#trainvalidation-splits).

* **Example Scripts for Microsoft Agents and CrewAI** - New example scripts demonstrating how to use Opik Optimizer with popular LLM frameworks. Check out the [example scripts](https://github.com/comet-ml/opik/tree/main/sdks/opik_optimizer/scripts/llm_frameworks).

* **UI Enhancements and Optimizer Improvements** - Several UI enhancements and various improvements to Few Shot, MetaPrompt, and GEPA optimizers for better usability and performance.

## ðŸŽ¨ User Experience Enhancements

Improved usability across the platform:

* **Added `has_tool_spans` Field to Show Tool Calls in Thread View** - Tool calls are now visible in thread views, providing better visibility into agent tool usage.

* **Added Export Capability (JSON/CSV) Directly from Trace, Thread, and Span Detail Views** - Export data directly from detail views in JSON or CSV format, making it easier to analyze and share your observability data.

## ðŸ¤– New Models!

Expanded model support:

* **Added Support for Gemini 3 Pro, GPT 5.1, OpenRouter Models** - Added support for the latest model versions including Gemini 3 Pro, GPT 5.1, and OpenRouter models, giving you access to the newest AI capabilities.

***

And much more! ðŸ‘‰ [See full commit log on GitHub](https://github.com/comet-ml/opik/compare/1.9.17...1.9.40)

*Releases*: `1.9.18`, `1.9.19`, `1.9.20`, `1.9.21`, `1.9.22`, `1.9.23`, `1.9.25`, `1.9.26`, `1.9.27`, `1.9.28`, `1.9.29`, `1.9.31`, `1.9.32`, `1.9.33`, `1.9.34`, `1.9.35`, `1.9.36`, `1.9.37`, `1.9.38`, `1.9.39`, `1.9.40`


# November 18, 2025

Here are the most relevant improvements we've made since the last release:

## ðŸ“Š More Metrics!

We have shipped **37 new built-in metrics**, faster & more reliable LLM judging, plus robustness fixes.

**New Metrics Added** - We've expanded the evaluation metrics library with a comprehensive set of out-of-the-box metrics including:

* **Classic NLP Heuristics** - BERTScore, Sentiment analysis, Bias detection, Conversation drift, and more
* **Lightweight Heuristics** - Fast, non-LLM based metrics perfect for CI/CD pipelines and large-scale evaluations
* **LLM-as-a-Judge Presets** - More out-of-the-box presets you can use without custom configuration

**LLM-as-a-Judge & G-Eval Improvements**:

* **Compatible with newer models** - Now works seamlessly with the latest model versions
* **Faster default judge** - Default judge is now `gpt-5-nano` for faster, more accurate evals
* **LLM Jury support** - Aggregate scores across multiple models/judges into a single ensemble score for more reliable evaluations

**Enhanced Preprocessing**:

* **Improved English text handling** - Better processing of English text to reduce false negatives
* **Better emoji handling** - Enhanced emoji processing for more accurate evaluations

**Robustness Improvements**:

* **Automatic retries** - LLM judge will retry on transient failures to avoid flaky test results
* **More reliable evaluation runs** - Faster, more consistent evaluation runs for CI and experiments

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-11-18/evaluation_metrics_overview.png" alt="Evaluation metrics overview showing conversation heuristic metrics and LLM as a Judge metrics" />
</Frame>

ðŸ‘‰ Access the metrics docs here: [Evaluation Metrics Overview](/docs/opik/evaluation/metrics/overview)

## ðŸ”’ Anonymizers - PII Information Redaction

We've added **support for PII (Personally Identifiable Information) redaction** before sending data to Opik. This helps you protect sensitive information while still getting the observability insights you need.

With anonymizers, you can:

* **Automatically redact PII** from traces and spans before they're sent to Opik
* **Configure custom anonymization rules** to match your specific privacy requirements
* **Maintain compliance** with data protection regulations
* **Protect sensitive data** without losing observability

ðŸ‘‰ Read the full docs: [Anonymizers](/docs/opik/production/anonymizers)

## ðŸš¨ New Alert Types

We've expanded our alerting capabilities with new alert types and improved functionality:

* **Experiment Finished Alert** - Get notified when an experiment completes, so you can review results immediately or trigger your CI/CD pipelines.
* **Cost Alerts** - Set thresholds for cost metrics and receive alerts when spending exceeds your limits
* **Latency Alerts** - Monitor response times and get notified when latency exceeds configured thresholds

These new alert types help you stay on top of your LLM application's performance and costs, enabling proactive monitoring and faster response to issues.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-11-18/cost_latency_alerts.png" alt="Cost and latency alerts configuration" />
</Frame>

ðŸ‘‰ Read more: [Alerts Guide](/docs/opik/production/alerts)

## ðŸŽ¥ Multimodal Support

We've significantly enhanced multimodal capabilities across the platform:

* **Video LLM-as-a-Judge** - Added support for Video LLM-as-a-Judge, enabling evaluation of video content in your traces

* **Video Cost Tracking** - Added cost tracking for video models, so you can monitor spending on video processing operations

* **Image support in LLM-as-a-Judge** - Both Python and TypeScript SDKs now support image processing in LLM-as-a-Judge evaluations, allowing you to evaluate traces containing images

These enhancements make it easier to build and evaluate multimodal applications that work with images and video content.

## ðŸ”Œ Custom AI Providers

We've improved support for custom AI providers with enhanced configuration options:

* **Multiple Custom Providers** - Set up multiple custom AI providers for use in the Playground and online scoring
* **Custom Headers Support** - Configure custom headers for your custom providers, giving you more flexibility in how you connect to enterprise AI services

## ðŸ§ª Enhanced Evals & Observability

We've added several improvements to make evaluation and observability more powerful:

* **Trace and Span Metadata in Datasets** - Ability to add trace and span metadata to datasets for advanced agent evaluation, enabling more sophisticated evaluation workflows
* **Tokens Breakdown Display** - Display tokens breakdown (input/output) in the trace view, giving you detailed visibility into token usage for each span and trace
* **Binary (Boolean) Feedback Scores** - New support for binary (Boolean) feedback scores, allowing you to capture simple yes/no or pass/fail evaluations

## ðŸŽ¨ UX Improvements

We've made several user experience enhancements across the platform:

* **Improved Pretty Mode** - Enhanced pretty mode for traces, threads, and annotation queues, making it easier to read and understand your data
* **Date Filtering for Traces, Threads, and Spans** - Added date filtering capabilities, allowing you to focus on specific time ranges when analyzing your data
* **New Optimization Runs Section** - Added a new optimization runs section to the home page, giving you quick access to your optimization results
* **Comet Debugger Mode** - Added Comet Debugger Mode with app version and connectivity status, helping you troubleshoot issues and understand your application's connection status. Read more about it [here](/docs/opik/faq#using-comet-debugger-mode-uibrowser)

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-11-18/comet_debugger_mode.png" alt="Comet Debugger Mode showing RTT, connectivity status, and Opik version" />
</Frame>

***

And much more! ðŸ‘‰ [See full commit log on GitHub](https://github.com/comet-ml/opik/compare/1.8.97...1.9.17)

*Releases*: `1.8.98`, `1.8.99`, `1.8.100`, `1.8.101`, `1.8.102`, `1.9.0`, `1.9.1`, `1.9.2`, `1.9.3`, `1.9.4`, `1.9.5`, `1.9.6`, `1.9.7`, `1.9.8`, `1.9.9`, `1.9.10`, `1.9.11`, `1.9.12`, `1.9.13`, `1.9.14`, `1.9.15`, `1.9.16`, `1.9.17`


# November 4, 2025

Here are the most relevant improvements we've made since the last release:

## ðŸš¨ Native Slack and PagerDuty Alerts

We now offer **native Slack and PagerDuty alert integrations**, eliminating the need for any middleware configuration. Set up alerts directly in Opik to receive notifications when important events happen in your workspace.

With native integrations, you can:

* **Configure Slack channels** directly from Opik settings
* **Set up PagerDuty incidents** without additional webhook setup
* **Receive real-time notifications** for errors, feedback scores, and critical events
* **Streamline your monitoring workflow** with built-in integrations

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/production/create_alert_form.png" alt="Create alert form" />
</Frame>

ðŸ‘‰ Read the full docs here - [Alerts Guide](/docs/opik/production/alerts)

## ðŸ–¼ï¸ Multimodal LLM-as-a-Judge Support for Visual Evaluation

LLM as a Judge metrics can now evaluate traces that contain images when using vision-capable models. This is useful for:

* **Evaluating image generation quality** - Assess the quality and relevance of generated images
* **Analyzing visual content** in multimodal applications - Evaluate how well your application handles visual inputs
* **Validating image-based responses** - Ensure your vision models produce accurate and relevant outputs

To reference image data from traces in your evaluation prompts:

* In the prompt editor, click the **"Images +"** button to add an image variable
* Map the image variable to the trace field containing image data using the Variable Mapping section

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/production/online_evaluation_rule_add_image.png" />
</Frame>

ðŸ‘‰ Read more: [Evaluating traces with images](/docs/opik/production/rules#evaluating-traces-with-images)

## âœ¨ Prompt Generator & Improver

We've launched the **Prompt Generator** and **Prompt Improver** â€” two AI-powered tools that help you create and refine prompts faster, directly inside the Playground.

Designed for non-technical users, these features automatically apply best practices from OpenAI, Anthropic, and Google, helping you craft clear, effective, and production-grade prompts without leaving the Playground.

### Why it matters

Prompt engineering is still one of the biggest bottlenecks in LLM development. With these tools, teams can:

* **Generate high-quality prompts** from simple task descriptions
* **Improve existing prompts** for clarity, specificity, and consistency
* **Iterate and test prompts seamlessly** in the Playground

### How it works

* **Prompt Generator** â†’ Describe your task in plain language; Opik creates a complete system prompt following proven design principles
* **Prompt Improver** â†’ Select an existing prompt; Opik enhances it following best practices

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/prompt_engineering/prompt_improvement.png" />
</Frame>

ðŸ‘‰ Read the full docs: [Prompt Generator & Improver](/docs/opik/prompt_engineering/improve)

## ðŸ”— Advanced Prompt Integration in Spans & Traces

We've implemented  **prompt integration into spans and traces**, creating a seamless connection between your Prompt Library, Traces, and the Playground.

You can now associate prompts directly with traces and spans using the `opik_context` module â€” so every execution is automatically tied to the exact prompt version used.

Understanding which prompt produced a given trace is key for users building both simple and advanced multi-prompt and multi-agent systems.

With this integration, you can:

* **Track which prompt version** was used in each function or span
* **Audit and debug prompts** directly from trace details
* **Reproduce or improve prompts** instantly in the Playground
* **Close the loop** between prompt design, observability, and iteration

Once added, your prompts appear in the trace details view â€” with links back to the Prompt Library and the Playground, so you can iterate in one click.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/prompt_engineering/prompt_opik_context_update.png" />
</Frame>

ðŸ‘‰ Read more: [Adding prompts to traces and spans](/docs/opik/prompt_engineering/prompt_management#adding-prompts-to-traces-and-spans)

## ðŸ§ª Better No-Code Experiment Capabilities in the Playground

We've introduced a series of improvements directly in the Playground to make experimentation easier and more powerful:

**Key enhancements:**

1. **Create or select datasets** directly from the Playground
2. **Create or select online score rules** - Ability to choose the ones that you want to use on each run
3. **Ability to pass dataset items to online score rules** - This enables reference-based experiments, where outputs are automatically compared to expected answers or ground truth, making objective evaluation simple
4. **One-click navigation to experiment results** - From the Playground, users can now:
   * Jump into the Single Experiment View to inspect metrics and examples in detail, or
   * Go to the Compare Experiments View to benchmark multiple runs side-by-side

## ðŸ“Š On-Demand Online Evaluation on Existing Traces and Threads

We've added **on-demand online evaluation** in Opik, letting users run metrics on already logged traces and threads â€” perfect for evaluating historical data or backfilling new scores.

### How it works

Select traces/threads, choose any online score rule (e.g., Moderation, Equals, Contains), and run evaluations directly from the UI â€” no code needed.

Results appear inline as feedback scores and are fully logged for traceability.

This enables:

* **Fast, no-code evaluation** of existing data
* **Easy retroactive measurement** of model and agent performance
* **Historical data analysis** without re-running traces

ðŸ‘‰ Read more: [Manual Evaluation](/docs/opik/tracing/annotate_traces#manual-evaluation)

## ðŸ¤– Agent Evaluation Guides

We've added two new comprehensive guides on evaluating agents:

### 1. Evaluating Agent Trajectories

This guide helps you evaluate that your agent is making the right tool calls before returning the final answer. It's fundamentally about evaluating and scoring what is happening within a trace.

ðŸ‘‰ Read the full guide: [Evaluating Agent Trajectories](/docs/opik/evaluation/evaluate_agent_trajectory)

### 2. Evaluating Multi-Turn Agents

Evaluating chatbots is tough because you need to evaluate not just a single LLM response but instead a conversation. This guide walks you through how you can use the new `opik.simulation.SimulatedUser` method to create simulated threads for your agent.

ðŸ‘‰ Read the full guide: [Evaluating Multi-Turn Agents](/docs/opik/evaluation/evaluate_multi_turn_agents)

These new docs significantly strengthen our agent evaluation feature-set and include diagrams to visualize how each evaluation strategy works.

## ðŸ“¦ Import/Export Commands

Added new command-line functions for importing and exporting Opik data: you can now export all traces, spans, datasets, prompts, and evaluation rules from a project to local JSON or CSV files. Also helps you import data from local JSON files into an existing project.

### Top use cases it is useful for

* **Migrate** - Move data between projects or environments
* **Backup** - Create local backups of your project data
* **Version control** - Track changes to your prompts and evaluation rules
* **Data portability** - Easily transfer your Opik workspace data

Read the full docs: [Import/Export Commands](/docs/opik/tracing/import_export_commands)

***

And much more! ðŸ‘‰ [See full commit log on GitHub](https://github.com/comet-ml/opik/compare/1.8.83...1.8.97)

*Releases*: `1.8.83`, `1.8.84`, `1.8.85`, `1.8.86`, `1.8.87`, `1.8.88`, `1.8.89`, `1.8.90`, `1.8.91`, `1.8.92`, `1.8.93`, `1.8.94`, `1.8.95`, `1.8.96`, `1.8.97`


# October 21, 2025

Here are the most relevant improvements we've made since the last release:

## ðŸš¨ Alerts

We've launched **Alerts** â€” a powerful way to get automated webhook notifications from your Opik workspace whenever important events happen (errors, feedback scores, prompt changes, and more). Opik now sends an HTTP POST to your endpoint with rich, structured event data you can route anywhere.

Now, you can make Opik a seamless part of your end-to-end workflows! With the new Alerts you can:

* **Spot production errors** in near-real time
* **Track feedback scores** to monitor model quality and user satisfaction
* **Audit prompt changes** across your workspace
* **Funnel events** into your existing workflows and CI/CD pipelines

And this is just v1.0! We'll keep adding events and advanced filtering, thresholds and more fine-grained control in future iterations, always based on community feedback.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-10-21/Alerts.png" alt="Alerts configuration interface showing webhook setup and event types" />
</Frame>

Read the full docs here - [Alerts Guide](/docs/opik/production/alerts)

## ðŸ–¼ï¸ Expanded Multimodal Image Support

We've added a better image support across our platform!

### What's new?

**1. Image Support in LLM as a Judge online Evaluations** - LLM as a Judge evaluations now support images alongside text, enabling you to evaluate vision models and multimodal applications. Upload images and get comprehensive feedback on both text and visual content.

**2. Enhanced Playground Experience** - The playground now supports image inputs, allowing you to test prompts with images before running full evaluations. Perfect for experimenting with vision models and multimodal prompts.

**3. Improved Data Display** - Base64 image previews in data tables, better image handling in trace views, and enhanced pretty formatting for multimodal content.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-10-21/EvaluateWithImages.png" alt="LLM Judge evaluation interface showing image support for multimodal evaluations" />
</Frame>

Links to official docs: [Evaluating traces with images](/docs/opik/production/rules#evaluating-traces-with-images) and [Using images in the Plaground](/docs/opik/prompt_engineering/playground#using-images-in-the-playground)

## Opik Optimizer Updates

**1. Support Multi-Metric Optimization** - Support for optimizing multiple metrics simultaneously with comprehensive frontend and backend changes. Read [more](/docs/opik/agent_optimization/optimization/define_metrics#compose-metrics)

**2. HRPO (Hierarchical Reflective Prompt Optimizer)** - New optimizer with self-reflective capabilities. Read more about it [here](/docs/opik/agent_optimization/algorithms/hierarchical_adaptive_optimizer)

## Enhanced Feedback & Annotation experience

**1. Improved Annotation Queue Export** - Enhanced export functionality for annotation queues: export your annotated data seamlessly for further analysis.

**2. Annotation Queue UX Enhancements**

* **Hotkeys Navigation** - Improved keyboard navigation throughout the interface for a fast annotation experience
* **Return to Annotation Queue Button** - Easy navigation back to annotation queues
* **Resume Functionality** - Continue annotation work where you left off
* **Queue Creation from Traces** - Create annotation queues directly from trace tables

**3. Inline Feedback Editing** - Quickly edit user feedback directly in data tables with our new inline editing feature. Hover over feedback cells to reveal edit options, making annotation workflows faster and more intuitive.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-10-21/InlineUserFeedback.png" alt="Inline feedback editing interface showing hover-triggered edit options in data tables" />
</Frame>

Read more about our [Annotation Queues](/docs/opik/evaluation/annotation_queues)

## User Experience Enhancements

**1. Dark Mode Refinements** - Improved dark mode styling across UI components for better visual consistency and user experience.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-10-21/DarkMode.png" alt="Dark mode interface showing improved styling and visual consistency across UI components" />
</Frame>

**2. Enhanced Prompt Readability** - Better formatting and display of long prompts in the interface, making them easier to read and understand.

**3. Improved Online Evaluation Page** - Added search, filtering, and sorting capabilities to the online evaluation page for better data management.

**4. Better token and cost control**

* **Thread Cost Display** - Show cost information in thread sidebar headers
* **Sum Statistics** - Display sum statistics for cost and token columns in the traces table.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-10-21/TotalCost.png" alt="Total cost display showing cost information in thread sidebar headers and sum statistics" />
</Frame>

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-10-21/TotalDuration.png" alt="Total duration display showing duration statistics and timing information" />
</Frame>

**5. Filter-Aware Metric Aggregation** - Better experiment item filtering in the experiments details tables for better data control.

**6. Pretty Mode Enhancements** - Improved the Pretty mode for Input/Output display with better formatting and readability across the product.

## TypeScript SDK Updates

* **Opik Configure Tool** - New `opik-ts` configure tool with a guided developer experience and local flag support
* **Prompt Management** - Comprehensive prompt management implementation
* **LangChain Integration** - Aligned LangChain integration with Python architecture

## Python SDK Improvements

* **Context Managers** - New context managers for span and trace creation
* **Bedrock Integration** - Enhanced Bedrock integration with invoke\_model support
* **Trace Updates** - New `update_trace()` method for easier trace modifications
* **Parallel Agent Support** - Support for logging parallel agents in ADK integration
* **Enhanced feedback score handling** with better category support

## Integration updates

**1. OpenTelemetry Improvements**

* **Thread ID Support** - Added support for thread\_id in OpenTelemetry endpoint
* **System Information in Telemetry** - Enhanced telemetry with system information

**2. Model Support Updates** - Added support for [Claude Haiku 4.5](https://www.anthropic.com/news/claude-haiku-4-5) and updated model pricing information across the platform.

And much more! ðŸ‘‰ [See full commit log on GitHub](https://github.com/comet-ml/opik/compare/1.8.62...1.8.83)

*Releases*: `1.8.63`, `1.8.64`, `1.8.65`, `1.8.66`, `1.8.67`, `1.8.68`, `1.8.69`, `1.8.70`, `1.8.71`, `1.8.72`, `1.8.73`, `1.8.74`, `1.8.75`, `1.8.76`, `1.8.77`, `1.8.78`, `1.8.79`, `1.8.80`, `1.8.81`, `1.8.82`, `1.8.83`


# October 3, 2025

Here are the most relevant improvements we've made since the last release:

## ðŸ“ Multi-Value Feedback Scores & Annotation Queues

We're excited to announce major improvements to our evaluation and annotation capabilities!

### What's new?

**1. Multi-Value Feedback Scores**
Multiple users can now independently score the same trace or thread. No more overwriting each other's inputâ€”every reviewer's perspective is preserved and is visible in the product. This enables richer, more reliable consensus-building during evaluation.

**2. Annotation Queues**
Create queues of traces or threads that need expert review. Share them with SMEs through simple links. Organize work systematically, track progress, and collect both structured and unstructured feedback at scale.

**3. Simplified Annotation Experience**
A clean, focused UI designed for non-technical reviewers. Support for clear instructions, predefined feedback metrics, and progress indicators. Lightweight and distraction-free, so SMEs can concentrate on providing high-quality feedback.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-10-03/AnnotationQueues.png" alt="Annotation Queues interface showing SME workflow and feedback collection" />
</Frame>

[Full Documentation: Annotation Queues](/docs/opik/evaluation/annotation_queues)

## ðŸš€ Opik Optimizer - GEPA Algorithm & MCP Tool Optimization

### What's new?

**1. GEPA (Genetic-Pareto) Support**
[GEPA](https://github.com/gepa-ai/gepa/) is the new algorithm for optimizing prompts from Stanford. This bolsters our existing optimizers with the latest algorithm to give users more options.

**2. MCP Tool Calling Optimization**
The ability to tune MCP servers (external tools used by LLMs). Our solution uses our existing algorithm (MetaPrompter) to use LLMs to tune how LLMs interact with an MCP tool. The final output is a new tool signature which you can commit back to your code.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-10-03/GEPAOptimizer.png" alt="GEPA Optimizer interface showing genetic-pareto algorithm for prompt optimization" />
</Frame>

[Full Documentation: Tool Optimization](/docs/opik/agent_optimization/algorithms/tool_optimization) | [GEPA Optimizer](/docs/opik/agent_optimization/algorithms/gepa_optimizer)

## ðŸ” Dataset & Search Enhancements

* Added dataset search and dataset items download functionality

## ðŸ Python SDK Improvements

* Implement granular support for choosing dataset items in experiments
* Better project name setting and onboarding
* Implement calculation of mean/min/max/std for each metric in experiments
* Update CrewAI to support CrewAI flows

## ðŸŽ¨ UX Enhancements

* Add clickable links in trace metadata
* Add description field to feedback definitions

And much more! ðŸ‘‰ [See full commit log on GitHub](https://github.com/comet-ml/opik/compare/1.8.42...1.8.62)

*Releases*: `1.8.43`, `1.8.44`, `1.8.45`, `1.8.46`, `1.8.47`, `1.8.48`, `1.8.49`, `1.8.50`, `1.8.51`, `1.8.52`, `1.8.53`, `1.8.54`, `1.8.55`, `1.8.56`, `1.8.57`, `1.8.58`, `1.8.59`, `1.8.60`, `1.8.61`, `1.8.62`


# September 5, 2025

Here are the most relevant improvements we've made since the last release:

## ðŸ” Opik Trace Analyzer Beta is Live!

We're excited to announce the launch of **Opik Trace Analyzer** on Opik Cloud!

What this means: faster debugging & analysis!

Our users can now easily understand, analyze, and debug their development and production traces.

Want to give it a try? All you need to do is go to one of your traces and click on "Inspect trace" to start getting valuable insights.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-09-05/opik_trace_analyzer.png" alt="Opik Trace Analyzer Beta interface showing trace analysis and debugging features" />
</Frame>

## âœ¨ Features and Improvements

* We've finally added **dark mode** support! This feature has been requested many times by our community members. You can now switch your theme in your account settings.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-09-05/dark_mode.png" alt="Dark mode theme toggle in Opik account settings showing light and dark theme options" />
</Frame>

* Now you can filter the widgets in the metrics tab by trace and threads attributes

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-09-05/metrics_filters.png" alt="Metrics tab filters showing trace and thread attribute filtering options" />
</Frame>

* Annotating tons of threads? We've added the ability to export feedback score comments for threads to CSV for easier analysis in external tools.
* We have also improved the discoverability of the experiment comparison feature.
* Added new filter operators to the Experiments table

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-09-05/experiment_operators.png" alt="Experiment table filter operators showing advanced filtering options" />
</Frame>

* Adding assets as part of your experiment's metadata? We now display clickable links in the experiment config tab for easier navigation.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-09-05/clickable_assets_metadata.png" alt="Clickable assets and metadata links in experiment configuration tab showing improved navigation" />
</Frame>

## ðŸ“š Documentation

* We've released [Opik University](/opik/opik-university)! This is a new section of the docs full of video guides explaining the product.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-09-05/opik_university.png" alt="Opik University documentation section showing video guides and tutorials" />
</Frame>

## ðŸ”Œ SDK & Integration Improvements

* Enhanced *LangChain* integration with comprehensive tests and build fixes
* Implemented new search\_prompts method in the Python SDK
* Added [documentation for models, providers, and frameworks supported for cost tracking](/docs/opik/tracing/cost_tracking#supported-models-providers-and-integrations)
* Enhanced Google ADK integration to log **error information to corresponding spans and traces**

And much more! ðŸ‘‰ [See full commit log on GitHub](https://github.com/comet-ml/opik/compare/1.8.33...1.8.42)

*Releases*: `1.8.34`, `1.8.35`, `1.8.36`, `1.8.37`, `1.8.38`, `1.8.39`, `1.8.40`, `1.8.41`, `1.8.42`


# August 22, 2025

Here are the most relevant improvements we've made in the last couple of weeks:

## ðŸ§ª Experiment Grouping

Instantly organize and compare experiments by model, provider, or custom metadata to surface top performers, identify slow configurations, and discover winning parameter combinations. The new Group by feature provides aggregated statistics for each group, making it easier to analyze patterns across hundreds of experiments.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-08-22/experiment-grouping.png" alt="Experiment Grouping Interface" />
</Frame>

## ðŸ¤– Expanded Model Support

Added support for 144+ new models, including:

* OpenAI's GPT-5 and GPT-4.1-mini
* Anthropic Claude Opus 4.1
* Grok 4
* DeepSeek v3
* Qwen 3

## ðŸ›« Streamlined Onboarding

New quick start experience with AI-assisted installation, interactive setup guides, and instant access to team collaboration features and support.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-08-22/onboarding.png" alt="New Onboarding Experience" />
</Frame>

## ðŸ”Œ Integrations

Enhanced support for leading AI frameworks including:

* **LangChain**: Improved token usage tracking functionality
* **Bedrock**: Comprehensive cost tracking for Bedrock models

## ðŸ” Custom Trace Filters

Advanced filtering capabilities with support for list-like keys in trace and span filters, enabling precise data segmentation and analysis across your LLM operations.

## âš¡ Performance Optimizations

* Python scoring performance improvements with pre-warming
* Optimized ClickHouse async insert parameters
* Improved deduplication for spans and traces in batches

## ðŸ› ï¸ SDK Improvements

* Python SDK configuration error handling improvements
* Added dataset & dataset item ID to evaluate task inputs
* Updated OpenTelemetry integration

And much more! ðŸ‘‰ [See full commit log on GitHub](https://github.com/comet-ml/opik/compare/1.8.16...1.8.33)

*Releases*: `1.8.16`, `1.8.17`, `1.8.18`, `1.8.19`, `1.8.20`, `1.8.21`, `1.8.22`, `1.8.23`, `1.8.24`, `1.8.25`, `1.8.26`, `1.8.27`, `1.8.28`, `1.8.29`, `1.8.30`, `1.8.31`, `1.8.32`, `1.8.33`


# August 1, 2025

## ðŸŽ¯ Advanced Filtering & Search Capabilities

We've expanded filtering and search capabilities to help you find and analyze data more effectively:

* **Custom Trace Filters**: Support for custom filters on input/output fields for traces and spans, allowing more precise data filtering
* **Enhanced Search**: Improved search functionality with better result highlighting and local search within code blocks
* **Better Search Results**: Enhanced search result highlighting and improved local search functionality within code blocks
* **Crash Filtering**: Fixed filtering issues for values containing special characters like `%` to prevent crashes
* **Dataset Filtering**: Added support for experiments filtering by datasetId and promptId

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-08-01/Filtering.png" alt="Filtering and Search Interface" />
</Frame>

## ðŸ“Š Metrics & Analytics Improvements

We've enhanced the metrics and analytics capabilities:

* **Thread Feedback Scores**: Added comprehensive thread feedback scoring system for better conversation quality assessment
* **Thread Duration Monitoring**: New duration widgets in the Metrics dashboard for monitoring conversation length trends
* **Online Evaluation Rules**: Added ability to enable/disable online evaluation rules for more flexible monitoring
* **Cost Optimization**: Reduced cost prompt queries to improve performance and reduce unnecessary API calls

## ðŸŽ¨ UX Enhancements

We've made several UX improvements to make the platform more intuitive and efficient:

* **Full-Screen Popup Improvements**: Enhanced the full-screen popup experience with better navigation and usability
* **Tag Component Optimization**: Made tag components smaller and more compact for better space utilization
* **Column Sorting**: Enabled sorting and filtering on all Prompt columns for better data organization
* **Multi-Item Tagging**: Added ability to add tags to multiple items in the Traces and Spans tables simultaneously

## ðŸ”Œ SDK, integrations and docs

* **LangChain Integration**: Enhanced LangChain integration with improved provider and model logging
* **Google ADK Integration**: Updated Google ADK integration with better graph building capabilities
* **Bedrock Integration**: Added comprehensive cost tracking support for ChatBedrock and ChatBedrockConverse

## ðŸ”’ Security & Stability Enhancements

We've implemented several security and stability improvements:

* **Dependency Updates**: Updated critical dependencies including MySQL connector, OpenTelemetry, and various security patches
* **Error Handling**: Improved error handling and logging across the platform
* **Performance Monitoring**: Enhanced NewRelic support for better performance monitoring
* **Sentry Integration**: Added more metadata about package versions to Sentry events for better debugging

And much more! ðŸ‘‰ [See full commit log on GitHub](https://github.com/comet-ml/opik/compare/1.8.7...1.8.16)

*Releases*: `1.8.7`, `1.8.8`, `1.8.9`, `1.8.10`, `1.8.11`, `1.8.12`, `1.8.13`, `1.8.14`, `1.8.15`, `1.8.16`


# July 18, 2025

## ðŸ§µ Thread-level LLMs-as-Judge

We now support **thread-level LLMs-as-a-Judge metrics**!

We've implemented **Online evaluation for threads**, enabling the evaluation of **entire conversations between humans and agents**.

This allows for scalable measurement of metrics such as user frustration, goal achievement, conversational turn quality, clarification request rates, alignment with user intent, and much more.

We've also implemented **Python metrics support for threads**, giving you full code control over metric definitions.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-07-18/ThreadOnlineScore.png" alt="Thread Online Score Interface" />
</Frame>

To improve visibility into trends and to help detect spikes in these metrics when the agent is running in production, weâ€™ve added Thread Feedback Scores and Thread Duration widgets to the Metrics dashboard.
These additions make it easier to monitor changes over time in live environments.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-07-18/ThreadMetrics.png" alt="Thread Metrics Interface" />
</Frame>

## ðŸ” Improved Trace Inspection Experience

Once youâ€™ve identified problematic sessions or traces, weâ€™ve made it easier to inspect and analyze them with the following improvements:

* Field Selector for Trace Tree: Quickly choose which fields to display in the trace view.
* Span Type Filter: Filter spans by type to focus on what matters.
* Improved Agent Graph: Now supports full-page view and zoom for easier navigation.
* Free Text Search: Search across traces and spans freely without constraints.
* Better Search Usability: search results are now highlighted and local search is available within code blocks.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-07-18/ThreadImprovements.png" alt="Thread Improvements Interface" />
</Frame>

## ðŸ“Š Spans Tab Improvements

The Spans tab provides a clearer, more comprehensive view of agent activity to help you analyze tool and sub-agent usage across threads, uncover trends, and spot latency outliers more easily.

Whatâ€™s New:

* LLM Calls â†’ Spans: weâ€™ve renamed the LLM Calls tab to Spans to reflect broader coverage and richer insights.
* Unified View: see all spans in one place, including LLM calls, tools, guardrails, and more.
* Span Type Filter: quickly filter spans by type to focus on what matters most.
* Customizable Columns: highlight key span types by adding them as dedicated columns.

These improvements make it faster and easier to inspect agent behavior and performance at a glance.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-07-18/SpansTableFilter.png" alt="Spans Table Filter Interface" />
</Frame>

## ðŸ“ˆ Experiments Improvements

Slow model response times can lead to frustrating user experiences and create hidden bottlenecks in production systems.
However, identifying latency issues early (during experimentation) is often difficult without clear visibility into model performance.

To help address this, weâ€™ve added Duration as a key metric for monitoring model latency in the Experiments engine.
You can now include Duration as a selectable column in both the Experiments and Experiment Details views.
This makes it easier to identify slow-responding models or configurations early, so you can proactively address potential performance risks before they impact users.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-07-18/ExperimentDuration.png" alt="Experiment Duration Interface" />
</Frame>

## ðŸ“¦ Enhanced Data Organization & Tagging

When usage grows and data volumes increase, effective data management becomes crucial.
We've added several capabilities to make team workflows easier:

* Tagging, filtering, and column sorting support for Prompts
* Tagging, filtering, and column sorting support for Datasets
* Ability to add tags to multiple items in the Traces and Spans tables

## ðŸ¤– New Models Support

We've added support for:

* OpenAI GPT-4.1 and GPT-4.1-mini models
* Anthropic Claude 4 Sonnet model

## ðŸŒ Integration Updates

We've enhanced several integrations:

* Build graph for Google ADK agents
* Update Langchain integration to log provider, model and usage when using Google Generative AI models
* Implement Groq LLM usage tracking support in the Langchain integration

And much more! ðŸ‘‰ [See full commit log on GitHub](https://github.com/comet-ml/opik/compare/1.8.0...1.8.6)

*Releases*: `1.8.0`, `1.8.1`, `1.8.2`, `1.8.3`, `1.8.4`, `1.8.5`, `1.8.6`


# July 4, 2025

## ðŸ›  Agent Optimizer 1.0 released!

The Opik Agent Optimizer now supports full agentic systems and not just single prompts.

With support for LangGraph, Google ADK, PydanticAI, and more, this release brings a simplified API, model customization for evaluation, and standardized interfaces to streamline optimization workflows. [Learn more in the docs.](/docs/opik/agent_optimization/overview)

## ðŸ§µ Thread-level improvements

Added **Thread-Level Feedback, Tags & Comments**: You can now add expert feedback scores directly at the thread level, enabling SMEs to review full agent conversations, flag risks, and collaborate with dev teams more effectively. Added support for thread-level tags and comments to streamline workflows and improve context sharing.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-07-04/Thread-level_human_feedback.png" />
</Frame>

## ðŸ–¥ï¸ UX improvements

* Weâ€™ve redesigned the **Opik Home Page** to deliver a cleaner, more intuitive first-use experience, with a focused value proposition, direct access to key metrics, and a polished look. The demo data has also been upgraded to showcase Opikâ€™s capabilities more effectively for new users. Additionally, we've added **inter-project comparison capabilities** for metrics and cost control, allowing you to benchmark and monitor performance and expenses across multiple projects.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-07-04/Home_Page_1.png" />
</Frame>

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-07-04/Home_Page_2.png" />
</Frame>

* **Improved Error Visualization**: Enhanced how span-level errors are surfaced across the project. Errors now bubble up to the project view, with quick-access shortcuts to detailed error logs and variation stats for better debugging and error tracking.

* **Improved Sidebar Hotkeys**: Updated sidebar hotkeys for more efficient keyboard navigation between items and detail views.

## ðŸ”Œ SDK, integrations and docs

* Added **Langchain** support in metric classes, allowing use of Langchain as a model proxy alongside LiteLLM for flexible LLM judge customization.
* Added support for the **Gemini 2.5** model family.
* Updated pretty mode to support **Dify** and **LangGraph + OpenAI** responses.
* Added the **OpenAI agents integration cookbook** ([link](/integrations/openai_agents)).
* Added a cookbook on how to import **Huggingface Datasets to Opik**

ðŸ‘‰ [See full commit log on GitHub](https://github.com/comet-ml/opik/compare/1.7.37...1.7.42)

*Releases*: `1.7.37`, `1.7.38`, `1.7.39`, `1.7.40`, `1.7.41`, `1.7.42`


# June 20, 2025

## ðŸ”Œ Integrations and SDK

* Added **CloudFlare's WorkersAI** integration ([docs](/docs/opik/integrations/cloudflare-workers-ai))
* **Google ADK** integration: tracing is now automatically propagated to all sub-agents in agentic systems with the new `track_adk_agent_recursive` feature, eliminating the need to manually add tracing to each sub-agent.
* **Google ADK** integration: now we retrieve session-level information from the ADK framework to enrich the threads data.
* **New in the SDK!** Real-time tracking for long-running spans/traces is now supported. When enabled (set `os.environ["OPIK_LOG_START_TRACE_SPAN"] = "True"` in your environment), you can see traces and spans update live in the UIâ€”even for jobs that are still running. This makes debugging and monitoring long-running agents much more responsive and convenient.

## ðŸ§µ Threads improvements

* Added **Token Count and Cost Metrics** in Thread table
* Added **Sorting on all Thread table columns**
* Added **Navigation** from Thread Detail to all related traces
* Added support for **"pretty mode"** in OpenAI Agents threads

## ðŸ§ª Experiments improvements

* Added support for filtering by **configuration metadata** to experiments. It is now also possible to add a new column displaying the configuration in the experiments table.

## ðŸ›  Agent Optimizer improvements

* New Public API for Agent Optimization
* Added optimization run display link
* Added `optimization_context`

## ðŸ›¡ï¸ Security Fixes

* Fixed: h11 accepted some malformed Chunked-Encoding bodies
* Fixed: setuptools had a path traversal vulnerability in PackageIndex.download that could lead to Arbitrary File Write
* Fixed: LiteLLM had an Improper Authorization Vulnerability

ðŸ‘‰ [See full commit log on GitHub](https://github.com/comet-ml/opik/compare/1.7.31...1.7.36)

*Releases*: `1.7.32`, `1.7.33`, `1.7.34`, `1.7.35`, `1.7.36`


# June 6, 2025

## ðŸ’¡ Product Enhancements

* Ability to upload **CSV datasets** directly through the user interface
* Add **experiment cost tracking** to the Experiments table
* Add hinters and helpers for **onboarding new users** across the platform
* Added "LLM calls count" to the traces table
* Pretty formatting for complex agentic threads
* Preview **support for MP3** files in the frontend

## ðŸ›  SDKs and API Enhancements

* Good news for JS developers! We've released **experiments support for the JS SDK** (official docs coming very soon)
* New Experiments Bulk API: a new API has been introduced for logging Experiments in bulk.
* Rate Limiting improvements both in the API and the SDK

## ðŸ”Œ Integrations

* Support for OpenAI o3-mini and Groq models added to the Playground
* OpenAI Agents: context awareness implemented and robustness improved. Improve thread handling
* Google ADK: added support for multi-agent integration
* LiteLLM: token and cost tracking added for SDK calls. Integration now compatible with opik.configure(...)

ðŸ‘‰ [See full commit log on GitHub](https://github.com/comet-ml/opik/compare/1.7.26...1.7.31)

*Releases*: `1.7.27`, `1.7.28`, `1.7.29`, `1.7.30`, `1.7.31`


# May 23, 2025

## âœ¨ New Features

* **Opik Agent Optimizer**: A comprehensive toolkit designed to enhance the performance and efficiency of your Large Language Model (LLM) applications. [Read more](/docs/opik/agent_optimization/overview)

* **Opik Guardrails**: Guardrails help you protect your application from risks inherent in LLMs. Use them to check the inputs and outputs of your LLM calls, and detect issues like off-topic answers or leaking sensitive information. [Read more](/docs/opik/production/guardrails)

## ðŸ’¡ Product Enhancements

* **New Prompt Selector in Playground** â€” Choose existing prompts from your Prompt Library to streamline your testing workflows.
* **Improved â€œPretty Formatâ€ for Agents** â€” Enhanced readability for complex threads in the UI.

## ðŸ”Œ Integrations

* **Vertex AI (Gemini)** â€” Offline and online evaluation support integrated directly into Opik. Also available now in the Playground.
* **OpenAI Integration in the JS/TS SDK**
* **AWS Strands Agents**
* **Agno Framework**
* **Google ADK Multi-agent support**

## ðŸ›  SDKs and API Enhancements

* **OpenAI LLM advanced configurations** â€” Support for custom headers and base URLs.
* **Span Timing Precision** â€” Time resolution improved to microseconds for accurate monitoring.
* **Better Error Messaging** â€” More descriptive errors for SDK validation and runtime failures.
* **Stream-based Tracing and Enhanced Streaming support**

ðŸ‘‰ [See full commit log on GitHub](https://github.com/comet-ml/opik/compare/1.7.18...1.7.26)

*Releases*: `1.7.19`, `1.7.20`, `1.7.21`, `1.7.22`, `1.7.23`, `1.7.24`, `1.7.25`, `1.7.26`


# May 5, 2025

**Opik Dashboard**:

**Python and JS / TS SDK**:

* Added support for streaming in ADK integration
* Add cost tracking for the ADK integration
* Add support for OpenAI `responses.parse`
* Reduce the memory and CPU overhead of the Python SDK through various
  performance optimizations

**Deployments**:

* Updated port mapping when using `opik.sh`
* Fixed persistence when using Docker compose deployments

*Release*: `1.7.15`, `1.7.16`, `1.7.17`, `1.7.18`


# April 28, 2025

**Opik Dashboard**:

* Updated the experiment page charts to better handle nulls, all metric values
  are now displayed.
* Added lazy loading for traces and span sidebar to better handle very large
  traces.
* Added support for trace and span attachments, you can now log pdf, video and
  audio files to your traces.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-04-28/trace_attachment.gif" />
</Frame>

* Improved performance of some Experiment endpoints

**Python and JS / TS SDK**:

* Updated DSPy integration following latest DSPy release
* New Autogen integration based on Opik's OpenTelemetry endpoints
* Added compression to request payload

*Release*: `1.7.12`, `1.7.13`, `1.7.14`


# April 21, 2025

**Opik Dashboard**:

* Released Python code metrics for online evaluations for both Opik Cloud and
  self-hosted deployments. This allows you to define python functions to evaluate
  your traces in production.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-04-21/python_code_metrics.png" />
</Frame>

**Python and JS / TS SDK**:

* Fixed LLM as a judge metrics so they return an error rather than a score of
  0.5 if the LLM returns a score that wasn't in the range 0 to 1.

**Deployments**:

* Updated Dockerfiles to ensure all containers run as non root users.

*Release*: `1.7.11`


# April 14, 2025

**Opik Dashboard:**

* Updated the feedback scores UI in the experiment page to make it easier to
  annotate experiment results.
* Fixed an issue with base64 encoded images in the experiment sidebar.
* Improved the loading speeds of the traces table and traces sidebar for traces
  that have very large payloads (25MB+).

**Python and JS / TS SDK**:

* Improved the robustness of LLM as a Judge metrics with better parsing.
* Fix usage tracking for Anthropic models hosted on VertexAI.
* When using LiteLLM, we fallback to using the LiteLLM cost if no model provider
  or model is specified.
* Added support for `thread_id` in the LangGraph integration.

*Releases*: `1.7.4`, `1.7.5`, `1.7.6`. `1.7.7` and `1.7.8`.


# April 7, 2025

**Opik Dashboard:**

* Added search to codeblocks in the input and output fields.
* Added sorting on feedback scores in the traces and spans tables:
  <Frame>
    <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-04-07/sort_feedback.gif" />
  </Frame>
* Added sorting on feedback scores in the experiments table.

**Python and JS / TS SDK**:

* Released a new integration with [Google ADK framework](https://google.github.io/adk-docs/).
* Cleanup up usage information by removing it from metadata field if it's already
  part of the `Usage` field.
* Added support for `Rouge` metric - Thanks @rohithmsr !
* Updated the LangChain callback `OpikTracer()` to log the data in a structured
  way rather than as raw text. This is expecially useful when using LangGraph.
* Updated the LangChainJS integration with additional examples and small fixes.
* Updated the OpenAI integration to support the Responses API.
* Introduced a new AggregatedMetric metric that can be used to compute aggregations
  of metrics in experiments.
* Added logging for LLamaIndex streaming methods.
* Added a new `text` property on the Opik.Prompt object.

*Releases*: `1.6.14`, `1.7.0`, `1.7.1`, `1.7.2`


# March 31, 2025

**Opik Dashboard:**

* Render markdown in experiment output sidebar
* The preference between pretty / JSON and YAML views are now saved
* We now hide image base64 strings in the traces sidebar to make it easier to read

**Python and JS / TS SDK**:

* Released a new [integration with Flowise AI](https://docs.flowiseai.com/using-flowise/analytics/opik)
* LangChain JS integration
* Added support for jinja2 prompts


# March 24, 2025

**General**

* Introduced a new `.opik.sh` installation script

**Opik Dashboard:**

* You can now view the number of spans for each trace in the traces table
* Add the option to search spans from the traces sidebar
* Improved performance of the traces table

**Python and JS / TS SDK**:

* Fixed issue related to log\_probs in Geval metric
* Unknown fields are no longer excluded when using the OpenTelemetry integration


# March 17, 2025

**Opik Dashboard:**

* We have revamped the traces table, the header row is now sticky at the top of
  the page when scrolling

  <Frame>
    <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-03-17/sticky_header.gif" />
  </Frame>

* As part of this revamp, we also made rows clickable to make it easier to open
  the traces sidebar

* Added visualizations in the experiment comparison page to help you analyze
  your experiments

  <Frame>
    <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-03-17/radar_chart.png" />
  </Frame>

* You can now filter traces by empty feedback scores in the traces table

* Added support for Gemini options in the playground

* Updated the experiment creation code

* Many performance improvements

**Python and JS / TS SDK**:

* Add support for Anthropic cost tracking when using the LangChain integration
* Add support for images in google.genai calls
* [LangFlow integration](https://github.com/langflow-ai/langflow/pull/6928) has now been merged


# March 10, 2025

**Opik Dashboard:**

* Add CSV export for the experiment comparison page

* Added a pretty mode for rendering trace and span input / output fields

  <Frame>
    <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-03-10/pretty_mode.png" />
  </Frame>

* Improved pretty mode to support new line characters and tabs

* Added time support for the Opik datetime filter

* Improved tooltips for long text

* Add `reason` field for feedback scores to json downloads

**Python and JS / TS SDK**:

* Day 0 integration with [OpenAI Agents](/integrations/openai_agents)
* Fixed issue with `get_experiment_by_name` method
* Added cost tracking for Anthropic integration
* Sped up the import time of the Opik library from \~5 seconds to less than 1 second


# March 3, 2025

**Opik Dashboard**:

* Chat conversations can now be reviewed in the platform

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-03-03/chat_conversations.png" />
</Frame>

* Added the ability to leave comments on experiments
* You can now leave reasons on feedback scores, see [Annotating Traces](/tracing/annotate_traces)
* Added support for Gemini in the playground
* A thumbs up / down feedback score definition is now added to all projects by default to make it easier
  to annotate traces.

**JS / TS SDK**:

* The AnswerRelevanceMetric can now be run without providing a context field
* Made some updates to how metrics are uploaded to optimize data ingestion


# February 24, 2025

**Opik Dashboard**:

* You can now add comments to your traces allowing for better collaboration:

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-02-24/comments_sidebar.png" />
</Frame>

* Added support for [OpenRouter](https://openrouter.ai/) in the playground - You can now use over 300 different
  models in the playground !

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-02-24/openrouter_playground.png" />
</Frame>

**JS / TS SDK**:

* Added support for JSON data format in our OpenTelemetry endpoints
* Added a new `opik healthcheck` command in the Python SDK which simplifies the debugging of connectivity issues


# February 17, 2025

**Opik Dashboard**:

* Improved the UX when navigating between the project list page and the traces page

**Python SDK**:

* Make the logging of spans and traces optional when using Opik LLM metrics
* New integration with genai library

**JS / TS SDK**:

* Added logs and better error handling


# February 10, 2025

**Opik Dashboard**:

* Added support for local models in the Opik playground

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-02-10/local_models.gif" />
</Frame>

**Python SDK**:

* Improved the `@track` decorator to better support nested generators.
* Added a new `Opik.copy_traces(project_name, destination_project_name)` method to copy traces
  from one project to another.
* Added support for searching for traces that have feedback scores with spaces in their name.
* Improved the LangChain and LangGraph integrations

**JS / TS SDK**:

* Released the Vercel AI integration
* Added support for logging feedback scores


# February 3, 2025

**Opik Dashboard**:

* You can now view feedback scores for your projects in the Opik home page
* Added line highlights in the quickstart page
* Allow users to download experiments as CSV and JSON files for further analysis

**Python SDK**:

* Update the `evaluate_*` methods so feedback scores are logged after they computed rather than at the end of an experiment as previously
* Released a new [usefulness metric](/evaluation/metrics/usefulness)
* Do not display warning messages about missing API key when Opik logging is disabled
* Add method to list datasets in a workspace
* Add method to list experiments linked to a dataset

**JS / TS SDK**:

* Official release of the first version of the SDK - Learn more [here](/tracing/log_traces#logging-with-the-js--ts-sdk)
* Support logging traces using the low-level Opik client and an experimental decorator.


# January 27, 2025

**Opik Dashboard**:

* Performance improvements for workspaces with 100th of millions of traces
* Added support for cost tracking when using Gemini models
* Allow users to diff prompt

**SDK**:

* Fixed the `evaluate` and `evaluate_*` functions to better support event loops, particularly useful when using Ragas metrics
* Added support for Bedrock `invoke_agent` API


# January 20, 2025

**Opik Dashboard**:

* Added logs for online evaluation rules so that you can more easily ensure your online evaluation metrics are working as expected
* Added auto-complete support in the variable mapping section of the online evaluation rules modal
* Added support for Anthropic models in the playground
* Experiments are now created when using datasets in the playground
* Improved the Opik home page
* Updated the code snippets in the quickstart to make them easier to understand

**SDK**:

* Improved support for litellm completion kwargs
* LiteLLM required version is now relaxed to avoid conflicts with other Python packages


# January 13, 2025

**Opik Dashboard**:

* Datasets are now supported in the playground allowing you to quickly evaluate prompts on multiple samples
* Updated the models supported in the playground
* Updated the quickstart guides to include all the supported integrations
* Fix issue that means traces with text inputs can't be added to datasets
* Add the ability to edit dataset descriptions in the UI
* Released [online evaluation](/production/rules) rules - You can now define LLM as a Judge metrics that will automatically score all, or a subset, of your production traces.

![Online evaluation](https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2025-01-13/online_evaluation.gif)

**SDK**:

* New integration with [CrewAI](/integrations/crewai)
* Released a new `evaluate_prompt` method that simplifies the evaluation of simple prompts templates
* Added Sentry to the Python SDK so we can more easily


# January 6, 2025

**Opik Dashboard**:

* Fixed an issue with the trace viewer in Safari

**SDK**:

* Added a new `py.typed` file to the SDK to make it compatible with mypy


# December 30, 2024

**Opik Dashboard**:

* Added duration chart to the project dashboard
* Prompt metadata can now be set and viewed in the UI, this can be used to store any additional information about the prompt
* Playground prompts and settings are now cached when you navigate away from the page

**SDK**:

* Introduced a new `OPIK_TRACK_DISABLE` environment variable to disable the tracking of traces and spans
* We now log usage information for traces logged using the LlamaIndex integration


# December 23, 2024

**SDK**:

* Improved error messages when getting a rate limit when using the `evaluate` method
* Added support for a new metadata field in the `Prompt` object, this field is used to store any additional information about the prompt.
* Updated the library used to create uuidv7 IDs
* New Guardrails integration
* New DSPY integration


# December 16, 2024

**Opik Dashboard**:

* The Opik playground is now in public preview
  <Frame>
    <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2024-12-16/playground.png" />
  </Frame>
* You can now view the prompt diff when updating a prompt from the UI
* Errors in traces and spans are now displayed in the UI
* Display agent graphs in the traces sidebar
* Released a new plugin for the [Kong AI Gateway](/production/gateway)

**SDK**:

* Added support for serializing Pydantic models passed to decorated functions
* Implemented `get_experiment_by_id` and `get_experiment_by_name` methods
* Scoring metrics are now logged to the traces when using the `evaluate` method
* New integration with [aisuite](/integrations/aisuite)
* New integration with [Haystack](/integrations/haystack)


# December 9, 2024

**Opik Dashboard**:

* Updated the experiments pages to make it easier to analyze the results of each experiment. Columns are now organized based on where they came from (dataset, evaluation task, etc) and output keys are now displayed in multiple columns to make it easier to review
  <Frame>
    <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2024-12-09/experiment_items_table.png" />
  </Frame>
* Improved the performance of the experiments so experiment items load faster
* Added descriptions for projects

**SDK**:

* Add cost tracking for OpenAI calls made using LangChain
* Fixed a timeout issue when calling `get_or_create_dataset`


# December 2, 2024

**Opik Dashboard**:

* Added a new `created_by` column for each table to indicate who created the record
* Mask the API key in the user menu

**SDK**:

* Implement background batch sending of traces to speed up processing of trace creation requests
* Updated OpenAI integration to track cost of LLM calls
* Updated `prompt.format` method to raise an error when it is called with the wrong arguments
* Updated the `Opik` method so it accepts the `api_key` parameter as a positional argument
* Improved the prompt template for the `hallucination` metric
* Introduced a new `opik_check_tls_certificate` configuration option to disable the TLS certificate check.


# November 25, 2024

**Opik Dashboard**:

* Feedback scores are now displayed as separate columns in the traces and spans table
* Introduce a new project dashboard to see trace count, feedback scores and token count over time.
  <Frame>
    <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2024-11-25/project_dashboard.png" />
  </Frame>
* Project statistics are now displayed in the traces and spans table header, this is especially useful for tracking the average feedback scores
  <Frame>
    <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2024-11-25/project_statistics.png" />
  </Frame>
* Redesigned the experiment item sidebar to make it easier to review experiment results
  <Frame>
    <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2024-11-25/experiment_item_sidebar.png" />
  </Frame>
* Annotating feedback scores in the UI now feels much faster
* Support exporting traces as JSON file in addition to CSV
* Sidebars now close when clicking outside of them
* Dataset groups in the experiment page are now sorted by last updated date
* Updated scrollbar styles for Windows users

**SDK**:

* Improved the robustness to connection issues by adding retry logic.
* Updated the OpenAI integration to track structured output calls using `beta.chat.completions.parse`.
* Fixed issue with `update_current_span` and `update_current_trace` that did not support updating the `output` field.


# November 18, 2024

**Opik Dashboard**:

* Updated the majority of tables to increase the information density, it is now easier to review many traces at once.
* Images logged to datasets and experiments are now displayed in the UI. Both images urls and base64 encoded images are supported.

**SDK**:

* The `scoring_metrics` argument is now optional in the `evaluate` method. This is useful if you are looking at evaluating your LLM calls manually in the Opik UI.
* When uploading a dataset, the SDK now prints a link to the dataset in the UI.
* Usage is now correctly logged when using the LangChain OpenAI integration.
* Implement a batching mechanism for uploading spans and dataset items to avoid `413 Request Entity Too Large` errors.
* Removed pandas and numpy as mandatory dependencies.


# November 11, 2024

**Opik Dashboard**:

* Added the option to sort the projects table by `Last updated`, `Created at` and `Name` columns.
* Updated the logic for displaying images, instead of relying on the format of the response, we now use regex rules to detect if the trace or span input includes a base64 encoded image or url.
* Improved performance of the Traces table by truncating trace inputs and outputs if they contain base64 encoded images.
* Fixed some issues with rendering trace input and outputs in YAML format.
* Added grouping and charts to the experiments page:
  <Frame>
    <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2024-11-11/experiment_summary.png" />
  </Frame>

**SDK**:

* **New integration**: Anthropic integration

  ```python wordWrap
  from anthropic import Anthropic, AsyncAnthropic
  from opik.integrations.anthropic import track_anthropic

  client = Anthropic()
  client = track_anthropic(client, project_name="anthropic-example")

  message = client.messages.create(
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": "Tell a fact",
            }
        ],
        model="claude-3-opus-20240229",
    )
  print(message)
  ```

* Added a new `evaluate_experiment` method in the SDK that can be used to re-score an existing experiment, learn more in the [Update experiments](/evaluation/update_existing_experiment) guide.


# November 4, 2024

**Opik Dashboard**:

* Added a new `Prompt library` page to manage your prompts in the UI.
  <Frame>
    <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2024-11-04/prompt_library_versions.png" />
  </Frame>

**SDK**:

* Introduced the `Prompt` object in the SDK to manage prompts stored in the library. See the [Prompt Management](/prompt_engineering/managing_prompts_in_code) guide for more details.
* Introduced a `Opik.search_spans` method to search for spans in a project. See the [Search spans](/tracing/export_data#exporting-spans) guide for more details.
* Released a new integration with [AWS Bedrock](/integrations/bedrock) for using Opik with Bedrock models.


# October 21, 2024

**Opik Dashboard**:

* Added the option to download traces and LLM calls as CSV files from the UI:
  <Frame>
    <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2024-10-21/download_traces.png" />
  </Frame>
* Introduce a new quickstart guide to help you get started:
  <Frame>
    <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2024-10-21/quickstart_guide.png" />
  </Frame>
* Updated datasets to support more flexible data schema, you can now insert items with any key value pairs and not just `input` and `expected_output`. See more in the SDK section below.
* Multiple small UX improvements (more informative empty state for projects, updated icons, feedback tab in the experiment page, etc).
* Fix issue with `\t` characters breaking the YAML code block in the traces page.

**SDK**:

* Datasets now support more flexible data schema, we now support inserting items with any key value pairs:

  ```python wordWrap
  import opik

  client = opik.Opik()
  dataset = client.get_or_create_dataset(name="Demo Dataset")
  dataset.insert([
      {
          "user_question": "Hello, what can you do ?",
          "expected_output": {
              "assistant_answer": "I am a chatbot assistant that can answer questions and help you with your queries!"
          }
      },
      {
          "user_question": "What is the capital of France?",
          "expected_output": {
              "assistant_answer": "Paris"
          }
      },
  ])
  ```

* Released WatsonX, Gemini and Groq integration based on the LiteLLM integration.

* The `context` field is now optional in the [Hallucination](/integrations/overview) metric.

* LLM as a Judge metrics now support customizing the LLM provider by specifying the `model` parameter. See more in the [Customizing LLM as a Judge metrics](/evaluation/metrics/overview#customizing-llm-as-a-judge-metrics) section.

* Fixed an issue when updating feedback scores using the `update_current_span` and `update_current_trace` methods. See this Github issue for more details.


# October 18, 2024

**Opik Dashboard**:

* Added a new `Feedback modal` in the UI so you can easily provide feedback on any parts of the platform.

**SDK**:

* Released new evaluation metric: [GEval](/evaluation/metrics/g_eval) - This LLM as a Judge metric is task agnostic and can be used to evaluate any LLM call based on your own custom evaluation criteria.
* Allow users to specify the path to the Opik configuration file using the `OPIK_CONFIG_PATH` environment variable, read more about it in the [Python SDK Configuration guide](/tracing/sdk_configuration#using-a-configuration-file).
* You can now configure the `project_name` as part of the `evaluate` method so that traces are logged to a specific project instead of the default one.
* Added a new `Opik.search_traces` method to search for traces, this includes support for a search string to return only specific traces.
* Enforce structured outputs for LLM as a Judge metrics so that they are more reliable (they will no longer fail when decoding the LLM response).


# October 14, 2024

**Opik Dashboard**:

* Fix handling of large experiment names in breadcrumbs and popups
* Add filtering options for experiment items in the experiment page
  <Frame>
    <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/changelog/2024-10-14/experiment_page_filtering.png" />
  </Frame>

**SDK:**

* Allow users to configure the project name in the LangChain integration


# October 7, 2024

**Opik Dashboard**:

* Added `Updated At` column in the project page
* Added support for filtering by token usage in the trace page

**SDK:**

* Added link to the trace project when traces are logged for the first time in a session
* Added link to the experiment page when calling the `evaluate` method
* Added `project_name` parameter in the `opik.Opik` client and `opik.track` decorator
* Added a new `nb_samples` parameter in the `evaluate` method to specify the number of samples to use for the evaluation
* Released the LiteLLM integration


# September 30, 2024

**Opik Dashboard**:

* Added option to delete experiments from the UI
* Updated empty state for projects with no traces
* Removed tooltip delay for the reason icon in the feedback score components

**SDK:**

* Introduced new `get_or_create_dataset` method to the `opik.Opik` client. This method will create a new dataset if it does not exist.
* When inserting items into a dataset, duplicate items are now silently ignored instead of being ingested.


# Tracing Core Concepts

> Learn about the core concepts of Opik's tracing system, including traces, spans, threads, and how they work together to provide comprehensive observability for your LLM applications.

<Tip>
  If you want to jump straight to logging traces, you can head to the [Log traces](/tracing/log_traces) or [Log agents](/tracing/log_agents) guides.
</Tip>

Tracing is the foundation of observability in Opik. It allows you to monitor, debug, and optimize your LLM applications by capturing detailed information about their execution. Understanding these core concepts is essential for effectively using Opik's tracing capabilities.

## Overview

When working with LLM applications, understanding what's happening under the hood is crucial for debugging issues, optimizing performance, and ensuring reliability. Opik's tracing system provides comprehensive observability by capturing detailed execution information at multiple levels.

In order to effectively use Opik's tracing capabilities, it's important to understand these key concepts:

1. **Trace**: A complete execution path representing a single interaction with an LLM or agent
2. **Span**: Individual operations or steps within a trace that represent specific actions or computations
3. **Thread**: A collection of related traces that form a coherent conversation or workflow
4. **Metric**: Quantitative measurements that provide objective assessments of your AI models' performance
5. **Optimization**: The systematic process of refining and evaluating LLM prompts and configurations
6. **Evaluation**: A framework for systematically testing your prompts and models against datasets

## Traces

A **trace** represents a complete execution path for a single interaction with an LLM or agent. Think of it as a detailed record of everything that happened during one request-response cycle. Each trace captures the full context of the interaction, including inputs, outputs, timing, and any intermediate steps.

### Key Characteristics of Traces:

* **Unique Identity**: Each trace has a unique identifier that allows you to track and reference it
* **Complete Context**: Contains all the information needed to understand what happened during the interaction
* **Timing Information**: Records when the interaction started, ended, and how long each part took
* **Input/Output Data**: Captures the exact prompts sent to the LLM and the responses received
* **Metadata**: Includes additional context like model used, temperature settings, and custom tags

### Example Use Cases:

* **Debugging**: When an LLM produces unexpected output, you can examine the trace to understand what went wrong
* **Performance Analysis**: Identify bottlenecks and slow operations by analyzing trace timing
* **Cost Tracking**: Monitor token usage and associated costs for each interaction
* **Quality Assurance**: Review traces to ensure your application is behaving as expected

## Spans

A **span** represents an individual operation or step within a trace. While a trace shows the complete picture, spans break down the execution into granular, measurable components. This hierarchical structure allows you to understand both the high-level flow and the detailed operations within your LLM application.

### Key Characteristics of Spans:

* **Hierarchical Structure**: Spans can contain other spans, creating a tree-like structure within a trace
* **Specific Operations**: Each span represents a distinct action, such as a function call, API request, or data processing step
* **Detailed Timing**: Precise start and end times for each operation
* **Context Preservation**: Maintains the relationship between parent and child operations
* **Custom Attributes**: Can include additional metadata specific to the operation

### Common Span Types:

* **LLM Calls**: Individual requests to language models
* **Function Calls**: Tool or function invocations within an agent
* **Data Processing**: Transformations or manipulations of data
* **External API Calls**: Requests to third-party services
* **Custom Operations**: Any user-defined operation you want to track

### Example Span Hierarchy:

```
Trace: "Customer Support Chat"
â”œâ”€â”€ Span: "Parse User Intent"
â”œâ”€â”€ Span: "Query Knowledge Base"
â”‚   â”œâ”€â”€ Span: "Search Vector Database"
â”‚   â””â”€â”€ Span: "Rank Results"
â”œâ”€â”€ Span: "Generate Response"
â”‚   â”œâ”€â”€ Span: "LLM Call: GPT-4"
â”‚   â””â”€â”€ Span: "Post-process Response"
â””â”€â”€ Span: "Log Interaction"
```

## Threads

A **thread** is a collection of related traces that form a coherent conversation or workflow. Threads are essential for understanding multi-turn interactions and maintaining context across multiple LLM calls. They provide a way to group related traces together, making it easier to analyze conversational patterns and user journeys.

### Key Characteristics of Threads:

* **Conversation Context**: Maintains the flow of multi-turn interactions
* **Trace Grouping**: Organizes related traces under a single thread identifier
* **Temporal Ordering**: Traces within a thread are ordered chronologically
* **Shared Context**: Allows you to see how context evolves throughout a conversation
* **Cross-Trace Analysis**: Enables analysis of patterns across multiple related interactions

### When to Use Threads:

* **Chat Applications**: Group all messages in a conversation
* **Multi-Step Workflows**: Track complex processes that span multiple LLM calls
* **User Sessions**: Organize all interactions from a single user session
* **Agent Conversations**: Follow the complete interaction between an agent and a user

### Thread Management:

Threads are created by defining a `thread_id` and referencing it in your traces. This allows you to:

* **Maintain Context**: Keep track of conversation history and user state
* **Debug Conversations**: Understand how a conversation evolved over time
* **Analyze Patterns**: Identify common conversation flows and user behaviors
* **Optimize Performance**: Find bottlenecks in multi-turn interactions

## Metrics

**Metrics** provide quantitative assessments of your AI models' outputs, enabling objective comparisons and performance tracking over time. They are essential for understanding how well your LLM applications are performing and identifying areas for improvement.

### Key Characteristics of Metrics:

* **Quantitative Measurement**: Provide numerical scores that can be compared and tracked
* **Objective Assessment**: Remove subjective bias from performance evaluation
* **Trend Analysis**: Enable tracking of performance changes over time
* **Comparative Analysis**: Allow comparison between different models, prompts, or configurations
* **Automated Evaluation**: Can be computed automatically without human intervention

### Common Metric Types:

* **Accuracy Metrics**: Measure how often the model produces correct outputs
* **Quality Metrics**: Assess the quality of generated text (e.g., coherence, relevance)
* **Efficiency Metrics**: Track performance characteristics like latency and throughput
* **Cost Metrics**: Monitor token usage and associated costs
* **Custom Metrics**: Domain-specific measurements tailored to your use case

## Optimization

**Optimization** is the systematic process of refining and evaluating LLM prompts and configurations to improve performance. It involves iteratively testing different approaches and using data-driven insights to make improvements.

### Key Aspects of Optimization:

* **Prompt Engineering**: Refining the instructions given to LLMs
* **Parameter Tuning**: Adjusting model settings like temperature, top-p, and max tokens
* **Few-shot Learning**: Optimizing example selection for in-context learning
* **Tool Integration**: Improving how LLMs interact with external tools and functions
* **Performance Monitoring**: Tracking improvements and regressions over time

## Evaluation

**Evaluation** provides a framework for systematically testing your prompts and models against datasets using various metrics to measure performance. It's the foundation for making data-driven decisions about your LLM applications.

### Key Components of Evaluation:

* **Datasets**: Collections of test cases with inputs and expected outputs
* **Experiments**: Individual evaluation runs that test specific configurations
* **Metrics**: Quantitative measures of performance
* **Comparative Analysis**: Side-by-side comparison of different approaches
* **Statistical Significance**: Ensuring results are reliable and reproducible

## Learn More

Now that you understand the core concepts, explore these resources to dive deeper:

### Tracing and Observability:

* [Log traces](/tracing/log_traces) - Learn how to capture traces in your applications
* [Log agents](/tracing/log_agents) - Understand how to trace agent-based applications
* [Annotate traces](/tracing/annotate_traces) - Add custom metadata to your traces
* [Cost tracking](/tracing/cost_tracking) - Monitor and analyze costs

### Evaluation and Testing:

* [Evaluation concepts](/evaluation/concepts) - Deep dive into evaluation concepts
* [Evaluate prompts](/evaluation/evaluate_prompt) - Test and compare different prompts
* [Evaluate agents](/evaluation/evaluate_agents) - Evaluate complex agent systems
* [Metrics overview](/evaluation/metrics/overview) - Available evaluation metrics

### Optimization:

* [Agent Optimization concepts](/agent_optimization/optimizer-concepts) - Core optimization concepts
* [Optimization algorithms](/agent_optimization/overview) - Available optimization strategies
* [Best practices](/agent_optimization/best_practices/prompt_engineering) - Optimization best practices

### Integration Guides:

* [SDK Configuration](/tracing/sdk_configuration) - Configure Opik in your applications
* [Supported Models](/tracing/supported_models) - Models compatible with Opik
* [Integrations](/integrations/overview) - Framework-specific integration guides

## Best Practices for Tracing

<Steps>
  <Step title="1. Start with Clear Trace Boundaries">
    Define clear boundaries for what constitutes a single trace. Typically, this should align with a complete user interaction or business operation.
  </Step>

  <Step title="2. Use Meaningful Span Names">
    Choose descriptive names for your spans that clearly indicate what operation is being performed. This makes debugging much easier.
  </Step>

  <Step title="3. Leverage Thread IDs for Conversations">
    Use consistent thread IDs for related interactions. This is especially important for chat applications and multi-step workflows.
  </Step>

  <Step title="4. Add Relevant Metadata">
    Include custom attributes and metadata that will be useful for analysis. Consider adding user IDs, session information, and business context.
  </Step>

  <Step title="5. Monitor Performance Continuously">
    Set up alerts and dashboards to monitor trace performance, error rates, and costs. This helps you catch issues early.
  </Step>

  <Step title="6. Use Traces for Optimization">
    Regularly analyze your traces to identify optimization opportunities, such as reducing latency or improving prompt effectiveness.
  </Step>
</Steps>

<Tip>
  **Pro Tip**: Start with basic tracing and gradually add more detailed spans as you identify areas that need deeper observability. Don't try to trace everything at once - focus on the most critical paths first.
</Tip>

<Warning>
  **Important**: Be mindful of sensitive data when tracing. Avoid logging personally identifiable information (PII) or sensitive business data in your traces. Use Opik's data filtering capabilities to protect sensitive information.
</Warning>


# Log traces

> Monitor the flow of your LLM applications with tracing to identify issues and optimize performance using Opik's powerful tools.

<Tip>
  If you are just getting started with Opik, we recommend first checking out the [Quickstart](/quickstart) guide that
  will walk you through the process of logging your first LLM call.
</Tip>

LLM applications are complex systems that do more than just call an LLM API, they will often involve retrieval, pre-processing and post-processing steps.
Tracing is a tool that helps you understand the flow of your application and identify specific points in your application that may be causing issues.

Opik's tracing functionality allows you to track not just all the LLM calls made by your application but also any of the other steps involved.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/tracing/introduction.png" />
</Frame>

Opik supports agent observability using our [Typescript SDK](/reference/typescript-sdk/overview),
[Python SDK](/reference/python-sdk/overview), [first class OpenTelemetry support](/integrations/opentelemetry)
and our [REST API](/reference/rest-api/overview).

<Tip>
  We recommend starting with one of our integrations to get started quickly, you can find a full list of our
  integrations in the [integrations overview](/integrations/overview) page.
</Tip>

We won't be covering how to track chat conversations in this guide, you can learn more about this in the
[Logging conversations](/tracing/log_chat_conversations) guide.

## Enable agent observability

### 1. Installing the SDK

Before adding observability to your application, you will first need to install and configure the
Opik SDK.

<Tabs>
  <Tab value="Typescript SDK" title="Typescript SDK" language="typescript">
    ```bash
    npm install opik
    ```

    You can then set the Opik environment variables in your `.env` file:

    ```bash
    # Set OPIK_API_KEY and OPIK_WORKSPACE_NAME in your .env file
    OPIK_API_KEY=your_api_key_here
    OPIK_WORKSPACE=your_workspace_name

    # Optional if you are using Opik Cloud:
    OPIK_URL_OVERRIDE=https://www.comet.com/opik/api
    ```
  </Tab>

  <Tab value="Python SDK" title="Python SDK" language="python">
    ```bash
    # Install the SDK
    pip install opik
    ```

    You can then configure the SDK using the `opik configure` CLI command or by calling
    [`opik.configure`](https://www.comet.com/docs/opik/python-sdk-reference/configure.html) from
    your Jupyter Notebook.
  </Tab>

  <Tab value="OpenTelemetry" title="OpenTelemetry">
    You will need to set the following environment variables for your OpenTelemetry setup:

    ```bash
    export OTEL_EXPORTER_OTLP_ENDPOINT=https://www.comet.com/opik/api/v1/private/otel
    export OTEL_EXPORTER_OTLP_HEADERS='Authorization=<your-api-key>,Comet-Workspace=default'

    # If you are using self-hosted instance:
    # export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:5173/api/v1/private/otel
    ```
  </Tab>
</Tabs>

<Tip>
  Opik is open-source and can be hosted locally using Docker, please refer to the [self-hosting
  guide](/self-host/overview) to get started. Alternatively, you can use our hosted platform by creating an account on
  [Comet](https://www.comet.com/signup?from=llm).
</Tip>

### 2. Using an integration

Once you have installed and configured the Opik SDK, you can start using it to track your agent calls:

<Tabs>
  <Tab title="OpenAI (TS)" value="openai-ts-sdk" language="typescript">
    If you are using the OpenAI TypeScript SDK, you can integrate by:

    <Steps>
      <Step>
        Install the Opik TypeScript SDK:

        ```bash
        npm install opik-openai
        ```
      </Step>

      <Step>
        Configure the Opik TypeScript SDK using environment variables:

        ```bash
        export OPIK_API_KEY="<your-api-key>" # Only required if you are using the Opik Cloud version
        export OPIK_URL_OVERRIDE="https://www.comet.com/opik/api" # Cloud version
        # export OPIK_URL_OVERRIDE="http://localhost:5173/api" # Self-hosting
        ```
      </Step>

      <Step>
        Wrap your OpenAI client with the `trackOpenAI` function:

        ```typescript
        import OpenAI from "openai";
        import { trackOpenAI } from "opik-openai";

        // Initialize the original OpenAI client
        const openai = new OpenAI({
          apiKey: process.env.OPENAI_API_KEY,
        });

        // Wrap the client with Opik tracking
        const trackedOpenAI = trackOpenAI(openai);

        // Use the tracked client just like the original
        const completion = await trackedOpenAI.chat.completions.create({
          model: "gpt-4",
          messages: [{ role: "user", content: "Hello, how can you help me today?" }],
        });
        console.log(completion.choices[0].message.content);

        // Ensure all traces are sent before your app terminates
        await trackedOpenAI.flush();
        ```

        All OpenAI calls made using the `trackedOpenAI` will now be logged to Opik.
      </Step>
    </Steps>
  </Tab>

  <Tab title="OpenAI (Python)" value="openai-python-sdk" language="python">
    If you are using the OpenAI Python SDK, you can integrate by:

    <Steps>
      <Step>
        Install the Opik Python SDK:

        ```bash
        pip install opik
        ```
      </Step>

      <Step>
        Configure the Opik Python SDK, this will prompt you for your API key if you are using Opik
        Cloud or your Opik server address if you are self-hosting:

        ```bash
        opik configure
        ```
      </Step>

      <Step>
        Wrap your OpenAI client with the `track_openai` function:

        ```python
        from opik.integrations.openai import track_openai
        from openai import OpenAI

        # Wrap your OpenAI client
        openai_client = OpenAI()
        openai_client = track_openai(openai_client)
        ```

        All OpenAI calls made using the `openai_client` will now be logged to Opik.
      </Step>
    </Steps>
  </Tab>

  <Tab title="AI Vercel SDK" value="ai-vercel-sdk" language="typescript">
    If you are using the AI Vercel SDK, you can integrate by:

    <Steps>
      <Step>
        Install the Opik Vercel integration:

        ```bash
        npm install opik-vercel
        ```
      </Step>

      <Step>
        Configure the Opik AI Vercel SDK using environment variables and set your Opik API key:

        ```bash
        export OPIK_API_KEY="<your-api-key>"
        export OPIK_URL_OVERRIDE="https://www.comet.com/opik/api" # Cloud version
        # export OPIK_URL_OVERRIDE="http://localhost:5173/api" # Self-hosting
        ```
      </Step>

      <Step>
        Initialize the OpikExporter with your AI SDK:

        ```ts
        import { openai } from "@ai-sdk/openai";
        import { generateText } from "ai";
        import { NodeSDK } from "@opentelemetry/sdk-node";
        import { getNodeAutoInstrumentations } from "@opentelemetry/auto-instrumentations-node";
        import { OpikExporter } from "opik-vercel";

        // Set up OpenTelemetry with Opik
        const sdk = new NodeSDK({
          traceExporter: new OpikExporter(),
          instrumentations: [getNodeAutoInstrumentations()],
        });
        sdk.start();

        // Your AI SDK calls with telemetry enabled
        const result = await generateText({
          model: openai("gpt-4o"),
          prompt: "What is love?",
          experimental_telemetry: { isEnabled: true },
        });

        console.log(result.text);
        ```

        All AI SDK calls with `experimental_telemetry: { isEnabled: true }` will now be logged to Opik.
      </Step>
    </Steps>
  </Tab>

  <Tab title="ADK" value="adk-python" language="python">
    If you are using the ADK, you can integrate by:

    <Steps>
      <Step>
        Install the Opik SDK:

        ```bash
        pip install opik
        ```
      </Step>

      <Step>
        Configure the Opik SDK by running the `opik configure` command in your terminal:

        ```bash
        opik configure
        ```
      </Step>

      <Step>
        Wrap your ADK agent with the `OpikTracer` decorator:

        ```python
        from opik.integrations.adk import OpikTracer, track_adk_agent_recursive

        opik_tracer = OpikTracer()

        # Define your ADK agent

        # Wrap your ADK agent with the OpikTracer
        track_adk_agent_recursive(agent, opik_tracer)
        ```

        All ADK agent calls will now be logged to Opik.
      </Step>
    </Steps>
  </Tab>

  <Tab title="LangGraph" value="langgraph" language="python">
    If you are using LangGraph, you can integrate by:

    <Steps>
      <Step>
        Install the Opik SDK:

        ```bash
        pip install opik
        ```
      </Step>

      <Step>
        Configure the Opik SDK by running the `opik configure` command in your terminal:

        ```bash
        opik configure
        ```
      </Step>

      <Step>
        Wrap your LangGraph graph with the `OpikTracer` decorator:

        ```python
        from opik.integrations.langchain import OpikTracer

        # Create your LangGraph graph
        graph = ...
        app = graph.compile(...)

        # Wrap your LangGraph graph with the OpikTracer
        opik_tracer = OpikTracer(graph=app.get_graph(xray=True))

        # Pass the OpikTracer callback to the invoke functions
        result = app.invoke({"messages": [HumanMessage(content = "How to use LangGraph ?")]},
                      config={"callbacks": [opik_tracer]})
        ```

        All LangGraph calls will now be logged to Opik.
      </Step>
    </Steps>
  </Tab>

  <Tab title="Function Decorators" value="python-function-decorator" language="python">
    If you are using the Python function decorator, you can integrate by:

    <Steps>
      <Step>
        Install the Opik Python SDK:

        ```bash
        pip install opik
        ```
      </Step>

      <Step>
        Configure the Opik Python SDK:

        ```bash
        opik configure
        ```
      </Step>

      <Step>
        Wrap your function with the `@track` decorator:

        ```python
        from opik import track

        @track
        def my_function(input: str) -> str:
            return input
        ```

        All calls to the `my_function` will now be logged to Opik. This works well for any function
        even nested ones and is also supported by most integrations (just wrap any parent function
        with the `@track` decorator).
      </Step>
    </Steps>
  </Tab>

  <Tab title="AI Wizard" value="ai-installation">
    <div>
      <span>
        <p>
          Integrate with Opik faster using this pre-built prompt
        </p>
      </span>

      <Button intent="primary" href="cursor:////anysphere.cursor-deeplink/prompt?text=%23+OPIK+Agentic+Onboarding%0A%0A%23%23+Goals%0A%0AYou+must+help+me%3A%0A%0A1.+Integrate+the+Opik+client+with+my+existing+LLM+application%0A2.+Set+up+tracing+for+my+LLM+calls+and+chains%0A%0A%23%23+Rules%0A%0ABefore+you+begin%2C+you+must+understand+and+strictly+adhere+to+these+core+principles%3A%0A%0A1.+Code+Preservation+%26+Integration+Guidelines%3A%0A%0A+++-+Existing+business+logic+must+remain+untouched+and+unmodified%0A+++-+Only+add+Opik-specific+code+%28decorators%2C+imports%2C+handlers%2C+env+vars%29%0A+++-+Integration+must+be+non-invasive+and+backwards+compatible%0A%0A2.+Process+Requirements%3A%0A%0A+++-+Follow+the+workflow+steps+sequentially+without+deviation%0A+++-+Validate+completion+of+each+step+before+proceeding%0A+++-+Request+explicit+approval+for+any+workflow+modifications%0A%0A3.+Documentation+%26+Resources%3A%0A%0A+++-+Reference+official+Opik+documentation+at+https%3A%2F%2Fwww.comet.com%2Fdocs%2Fopik%2Fquickstart.md%0A+++-+Follow+Opik+best+practices+and+recommended+patterns%0A+++-+Maintain+detailed+integration+notes+and+configuration+details%0A%0A4.+Testing+%26+Validation%3A%0A+++-+Verify+Opik+integration+without+impacting+existing+functionality%0A+++-+Validate+tracing+works+correctly+for+all+LLM+interactions%0A+++-+Ensure+proper+error+handling+and+logging%0A%0A%23%23+Integration+Workflow%0A%0A%23%23%23+Step+1%3A+Language+and+Compatibility+Check%0A%0AFirst%2C+analyze+the+codebase+to+identify%3A%0A%0A1.+Primary+programming+language+and+frameworks%0A2.+Existing+LLM+integrations+and+patterns%0A%0ACompatibility+Requirements%3A%0A%0A-+Supported+Languages%3A+Python%2C+JavaScript%2FTypeScript%0A%0AIf+the+codebase+uses+unsupported+languages%3A%0A%0A-+Stop+immediately%0A-+Inform+me+that+the+codebase+is+unsupported+for+AI+integration%0A%0AOnly+proceed+to+Step+2+if%3A%0A%0A-+Language+is+Python+or+JavaScript%2FTypeScript%0A%0A%23%23%23+Step+2%3A+Codebase+Discovery+%26+Entrypoint+Confirmation%0A%0AAfter+verifying+language+compatibility%2C+perform+a+full+codebase+scan+with+the+following+objectives%3A%0A%0A-+LLM+Touchpoints%3A+Locate+all+files+and+functions+that+invoke+or+interface+with+LLMs+or+can+be+a+candidates+for+tracing.%0A-+Entrypoint+Detection%3A+Identify+the+primary+application+entry+point%28s%29+%28e.g.%2C+main+script%2C+API+route%2C+CLI+handler%29.+If+ambiguous%2C+pause+and+request+clarification+on+which+component%28s%29+are+most+important+to+trace+before+proceeding.%0A++%E2%9A%A0%EF%B8%8F+Do+not+proceed+to+Step+3+without+explicit+confirmation+if+the+entrypoint+is+unclear.%0A-+Return+the+LLM+Touchpoints+to+me%0A%0A%23%23%23+Step+3%3A+Discover+Available+Integrations%0A%0AAfter+I+confirm+the+LLM+Touchpoints+and+entry+point%2C+find+the+list+of+supported+integrations+at+https%3A%2F%2Fwww.comet.com%2Fdocs%2Fopik%2Fintegrations%2Foverview.md%0A%0A%23%23%23+Step+4%3A+Deep+Analysis+Confirmed+files+for+LLM+Frameworks+%26+SDKs%0A%0AUsing+the+files+confirmed+in+Step+2%2C+perform+targeted+inspection+to+detect+specific+LLM-related+technologies+in+use%2C+such+as%3A%0ASDKs%3A+openai%2C+anthropic%2C+huggingface%2C+etc.%0AFrameworks%3A+LangChain%2C+LlamaIndex%2C+Haystack%2C+etc.%0A%0A%23%23%23+Step+5%3A+Pre-Implementation+Development+Plan+%28Approval+Required%29%0A%0ADo+not+write+or+modify+code+yet.+You+must+propose+me+a+step-by-step+plan+including%3A%0A%0A-+Opik+packages+to+install%0A-+Files+to+be+modified%0A-+Code+snippets+for+insertion%2C+clearly+scoped+and+annotated%0A-+Where+to+place+Opik+API+keys%2C+with+placeholder+comments+%28Visit+https%3A%2F%2Fcomet.com%2Fopik%2Fyour-workspace-name%2Fget-started+to+copy+your+API+key%29%0A++Wait+for+approval+before+proceeding%21%0A%0A%23%23%23+Step+6%3A+Execute+the+Integration+Plan%0A%0AAfter+approval%3A%0A%0A-+Run+the+package+installation+command+via+terminal+%28pip+install+opik%2C+npm+install+opik%2C+etc.%29.%0A-+Apply+code+modifications+exactly+as+described+in+Step+5.%0A-+Keep+all+additions+minimal+and+non-invasive.%0A++Upon+completion%2C+review+the+changes+made+and+confirm+installation+success.%0A%0A%23%23%23+Step+7%3A+Request+User+Review+and+Wait%0A%0ANotify+me+that+all+integration+steps+are+complete.%0A%22Please+run+the+application+and+verify+if+Opik+is+capturing+traces+as+expected.+Let+me+know+if+you+need+adjustments.%22%0A%0A%23%23%23+Step+8%3A+Debugging+Loop+%28If+Needed%29%0A%0AIf+issues+are+reported%3A%0A%0A1.+Parse+the+error+or+unexpected+behavior+from+feedback.%0A2.+Re-query+the+Opik+docs+using+https%3A%2F%2Fwww.comet.com%2Fdocs%2Fopik%2Fquickstart.md+if+needed.%0A3.+Propose+a+minimal+fix+and+await+approval.%0A4.+Apply+and+revalidate.%0A">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" id="Ebene_1" version="1.1" viewBox="0 0 466.73 532.09">
            <path class="st0" d="M457.43,125.94L244.42,2.96c-6.84-3.95-15.28-3.95-22.12,0L9.3,125.94c-5.75,3.32-9.3,9.46-9.3,16.11v247.99c0,6.65,3.55,12.79,9.3,16.11l213.01,122.98c6.84,3.95,15.28,3.95,22.12,0l213.01-122.98c5.75-3.32,9.3-9.46,9.3-16.11v-247.99c0-6.65-3.55-12.79-9.3-16.11h-.01ZM444.05,151.99l-205.63,356.16c-1.39,2.4-5.06,1.42-5.06-1.36v-233.21c0-4.66-2.49-8.97-6.53-11.31L24.87,145.67c-2.4-1.39-1.42-5.06,1.36-5.06h411.26c5.84,0,9.49,6.33,6.57,11.39h-.01Z" />
          </svg>

          Open in Cursor
        </div>
      </Button>
    </div>

    The pre-built prompt will guide you through the integration process, install the Opik SDK and
    instrument your code. It supports both Python and TypeScript codebases, if you are using
    another language just let us know and we can help you out.

    Once the integration is complete, simply run your application and you will start seeing traces
    in your Opik dashboard.
  </Tab>

  <Tab title="Other" value="other" language="other">
    Opik has more than 30 integrations with the most popular frameworks and libraries, you can find
    a full list of integrations [here](/integrations/overview). For example:

    * [Dify](/integrations/dify)
    * [Agno](/integrations/agno)
    * [Ollama](/integrations/ollama)

    If you are using a framework or library that is not listed, you can still log your traces
    using either the function decorator or the Opik client, check out the
    [Log Traces](/tracing/log_traces) guide for more information.
  </Tab>
</Tabs>

<Tip>
  Opik has more than 40 integrations with the majority of the popular frameworks and libraries. You can find a full list
  of integrations in the integrations [overview page](/integrations/overview).
</Tip>

If you would like more control over the logging process, you can use the low-level SDKs to log
your traces and spans.

### 3. Analyzing your agents

Now that you have observability enabled for your agents, you can start to review and analyze the
agent calls in Opik. In the Opik UI, you can review each agent call, see the
[agent graph](/tracing/log_agent_graph) and review all the tool calls made by the agent.

<Frame>
  <img src="https://files.buildwithfern.com/https://opik.docs.buildwithfern.com/docs/opik/2026-02-05T11:25:02.604Z/img/tracing/tracing_agent_overview.png" />
</Frame>

As a next step, you can create an [offline evaluation](/evaluation/evaluate_prompt) to evaluate your
agent's performance on a fixed set of samples.

## Advanced usage

### Using function decorators

Function decorators are a great way to add Opik logging to your existing application. When you add
the `@track` decorator to a function, Opik will create a span for that function call and log the
input parameters and function output for that function. If we detect that a decorated function
is being called within another decorated function, we will create a nested span for the inner
function.

While decorators are most popular in Python, we also support them in our Typescript SDK:

<Tabs>
  <Tab title="Typescript" value="typescript" language="typescript">
    TypeScript started supporting decorators from version 5 but it's use is still not widespread.
    The Opik typescript SDK also supports decorators but it's currently considered experimental.

    ```typescript maxLines=100
    import { track } from "opik";

    class TranslationService {
        @track({ type: "llm" })
        async generateText() {
            // Your LLM call here
            return "Generated text";
        }

        @track({ name: "translate" })
        async translate(text: string) {
            // Your translation logic here
            return `Translated: ${text}`;
        }

        @track({ name: "process", projectName: "translation-service" })
        async process() {
            const text = await this.generateText();
            return this.translate(text);
        }
    }
    ```

    <Info>
      You can also specify custom `tags`, `metadata`, and/or a `thread_id` for each trace and/or
      span logged for the decorated function. For more information, see
      [Logging additional data using the opik\_args parameter](#logging-additional-data)
    </Info>
  </Tab>

  <Tab title="Python" value="python" language="python">
    You can add the `@track` decorator to any function in your application and track not just
    LLM calls but also any other steps in your application:

    ```python maxLines=100
    import opik
    import openai

    client = openai.OpenAI()

    @opik.track
    def retrieve_context(input_text):
        # Your retrieval logic here, here we are just returning a
        # hardcoded list of strings
        context =[
            "What specific information are you looking for?",
            "How can I assist you with your interests today?",
            "Are there any topics you'd like to explore?",
        ]
        return context

    @opik.track
    def generate_response(input_text, context):
        full_prompt = (
            f" If the user asks a non-specific question, use the context to provide a relevant response.\n"
            f"Context: {', '.join(context)}\n"
            f"User: {input_text}\n"
            f"AI:"
        )

        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": full_prompt}]
        )
        return response.choices[0].message.content

    @opik.track(name="my_llm_application")
    def llm_chain(input_text):
        context = retrieve_context(input_text)
        response = generate_response(input_text, context)

        return response

    # Use the LLM chain
    result = llm_chain("Hello, how are you?")
    print(result)
    ```

    When using the track decorator, you can customize the data associated with both the trace
    and the span using either the `opik_args` parameter or the
    [`opik_context`](https://www.comet.com/docs/opik/python-sdk-reference/opik_context/index.html)
    module. This is particularly useful if you want to specify the conversation thread id, tags
    and metadata for example.

    <CodeBlocks>
      ```python title="opik_context module"
      import opik

      @opik.track
      def llm_chain(text: str) -> str:
          opik_context.update_current_trace(
              tags=["llm_chatbot"],
              metadata={"version": "1.0", "method": "simple"},
              thread_id="conversation-123",
              feedback_scores=[
                  {
                      "name": "user_feedback",
                      "value": 1
                  }
              ],
          )
          opik_context.update_current_span(
              metadata={"model": "gpt-4o"},
          )
          return f"Processed: {text}"
      ```

      ```python title="opik_args parameter"
      import opik

      @opik.track
      def llm_chain(text: str) -> str:
          # LLM chain code
          # ...
          return f"Processed: {text}"

      # Call with opik_args - it won't be passed to the function
      result = llm_chain(
          "hello world",
          opik_args={
              "span": {
                  "tags": ["llm", "agent"],
                  "metadata": {"version": "1.0", "method": "simple"}
              },
              "trace": {
                  "thread_id": "conversation-123",
                  "tags": ["user-session"],
                  "metadata": {"user_id": "user-456"}
              }
          }
      )

      print(result)
      ```
    </CodeBlocks>

    <Tip>
      If you specify the opik\_args parameter as part of your function call, you can propagate
      the configuration to the nested functions.
    </Tip>
  </Tab>
</Tabs>

### Using the low-level SDKs

If you need full control over the logging process, you can use the low-level SDKs to log your traces and spans:

<Tabs>
  <Tab title="Typescript" value="typescript" language="typescript">
    You can use the [`Opik`](/reference/typescript-sdk/overview) client to log your traces and spans:

    ```typescript
    import { Opik } from "opik";

    const client = new Opik({
        apiUrl: "https://www.comet.com/opik/api",
        apiKey: "your-api-key", // Only required if you are using Opik Cloud
        projectName: "your-project-name",
        workspaceName: "your-workspace-name", // Only required if you are using Opik Cloud
    });

    // Log a trace with an LLM span
    const trace = client.trace({
        name: `Trace`,
        input: {
            prompt: `Hello!`,
        },
        output: {
            response: `Hello, world!`,
        },
    });

    const span = trace.span({
        name: `Span`,
        type: "llm",
        input: {
            prompt: `Hello, world!`,
        },
        output: {
            response: `Hello, world!`,
        },
    });

    // Flush the client to send all traces and spans
    await client.flush();
    ```

    <Tip>
      Make sure you define the environment variables for the Opik client in your `.env` file,
      you can find more information about the configuration [here](/tracing/sdk_configuration).
    </Tip>
  </Tab>

  <Tab title="Python" value="python" language="python">
    If you want full control over the data logged to Opik, you can use the
    [`Opik`](https://www.comet.com/docs/opik/python-sdk-reference/Opik.html) client.

    Logging traces and spans can be achieved by first creating a trace using
    [`Opik.trace`](https://www.comet.com/docs/opik/python-sdk-reference/Opik.html#opik.Opik.trace)
    and then adding spans to the trace using the
    [`Trace.span`](https://www.comet.com/docs/opik/python-sdk-reference/Objects/Trace.html#opik.api_objects.trace.Trace.span)
    method:

    ```python
    from opik import Opik

    client = Opik(project_name="Opik client demo")

    # Create a trace
    trace = client.trace(
        name="my_trace",
        input={"user_question": "Hello, how are you?"},
        output={"response": "Comment Ã§a va?"}
    )

    # Add a span
    trace.span(
        name="Add prompt template",
        input={"text": "Hello, how are you?", "prompt_template": "Translate the following text to French: {text}"},
        output={"text": "Translate the following text to French: hello, how are you?"}
    )

    # Add an LLM call
    trace.span(
        name="llm_call",
        type="llm",
        input={"prompt": "Translate the following text to French: hello, how are you?"},
        output={"response": "Comment Ã§a va?"}
    )

    # End the trace
    trace.end()
    ```

    <Note>
      It is recommended to call `trace.end()` and `span.end()` when you are finished with the trace and span to ensure that
      the end time is logged correctly.
    </Note>

    Opik's logging functionality is designed with production environments in mind. To optimize
    performance, all logging operations are executed in a background thread.

    If you want to ensure all traces are logged to Opik before exiting your program, you can use the `opik.Opik.flush` method:

    ```python
    from opik import Opik

    client = Opik()

    # Log some traces
    client.flush()
    ```
  </Tab>
</Tabs>

### Logging traces/spans using context managers

If you are using the low-level SDKs, you can use the context managers to log traces and spans. Context managers provide a clean and Pythonic way to manage the lifecycle of traces and spans, ensuring proper cleanup and error handling.

<Tabs>
  <Tab title="Python" value="python" language="python">
    Opik provides two main context managers for logging:

    #### `opik.start_as_current_trace()`

    Use this context manager to create and manage a trace. A trace represents the overall execution flow of your application.

    For detailed API reference, see [`opik.start_as_current_trace`](https://www.comet.com/docs/opik/python-sdk-reference/context_manager/start_as_current_trace.html).

    ```python
    import opik

    # Basic trace creation
    with opik.start_as_current_trace("my-trace", project_name="my-project") as trace:
        # Your application logic here
        trace.input = {"user_query": "What is the weather?"}
        trace.output = {"response": "It's sunny today!"}
        trace.tags = ["weather", "api-call"]
        trace.metadata = {"model": "gpt-4", "temperature": 0.7}
    ```

    **Parameters:**

    * `name` (str): The name of the trace
    * `input` (Dict\[str, Any], optional): Input data for the trace
    * `output` (Dict\[str, Any], optional): Output data for the trace
    * `tags` (List\[str], optional): Tags to categorize the trace
    * `metadata` (Dict\[str, Any], optional): Additional metadata
    * `project_name` (str, optional): Project name (defaults to environment variable)
    * `thread_id` (str, optional): Thread identifier for multi-threaded applications
    * `flush` (bool, optional): Whether to flush data immediately (default: False)

    #### `opik.start_as_current_span()`

    Use this context manager to create and manage a span within a trace. Spans represent individual operations or function calls.

    For detailed API reference, see [`opik.start_as_current_span`](https://www.comet.com/docs/opik/python-sdk-reference/context_manager/start_as_current_span.html).

    ```python
    import opik

    # Basic span creation
    with opik.start_as_current_span("llm-call", type="llm", project_name="my-project") as span:
        # Your LLM call here
        span.input = {"prompt": "Explain quantum computing"}
        span.output = {"response": "Quantum computing is..."}
        span.model = "gpt-4"
        span.provider = "openai"
        span.usage = {
            "prompt_tokens": 10,
            "completion_tokens": 50,
            "total_tokens": 60
        }
    ```

    **Parameters:**

    * `name` (str): The name of the span
    * `type` (SpanType, optional): Type of span ("general", "tool", "llm", "guardrail", etc.)
    * `input` (Dict\[str, Any], optional): Input data for the span
    * `output` (Dict\[str, Any], optional): Output data for the span
    * `tags` (List\[str], optional): Tags to categorize the span
    * `metadata` (Dict\[str, Any], optional): Additional metadata
    * `project_name` (str, optional): Project name
    * `model` (str, optional): Model name for LLM spans
    * `provider` (str, optional): Provider name for LLM spans
    * `flush` (bool, optional): Whether to flush data immediately

    #### Nested Context Managers

    You can nest spans within traces to create hierarchical structures:

    ```python
    import opik

    with opik.start_as_current_trace("chatbot-conversation", project_name="chatbot") as trace:
        trace.input = {"user_message": "Help me with Python"}
        
        # First span: Process user input
        with opik.start_as_current_span("process-input", type="general") as span:
            span.input = {"raw_input": "Help me with Python"}
            span.output = {"processed_input": "Python programming help request"}
        
        # Second span: Generate response
        with opik.start_as_current_span("generate-response", type="llm") as span:
            span.input = {"prompt": "Python programming help request"}
            span.output = {"response": "I'd be happy to help with Python!"}
            span.model = "gpt-4"
            span.provider = "openai"
        
        trace.output = {"final_response": "I'd be happy to help with Python!"}
    ```

    #### Error Handling

    Context managers automatically handle errors and ensure proper cleanup:

    ```python
    import opik

    try:
        with opik.start_as_current_trace("risky-operation", project_name="my-project") as trace:
            trace.input = {"data": "important data"}
            # This will raise an exception
            result = 1 / 0
            trace.output = {"result": result}
    except ZeroDivisionError:
        # The trace is still properly closed and logged
        print("Error occurred, but trace was logged")
    ```

    #### Dynamic Parameter Updates

    You can modify trace and span parameters both inside and outside the context manager:

    ```python
    import opik

    # Parameters set outside the context manager
    with opik.start_as_current_trace(
        "dynamic-trace",
        input={"initial": "data"},
        tags=["initial-tag"],
        project_name="my-project"
    ) as trace:
        # Override parameters inside the context manager
        trace.input = {"updated": "data"}
        trace.tags = ["updated-tag", "new-tag"]
        trace.metadata = {"custom": "metadata"}
        
        # The final trace will use the updated values
    ```

    #### Flush Control

    Control when data is sent to Opik:

    ```python
    import opik

    # Immediate flush
    with opik.start_as_current_trace("immediate-trace", flush=True) as trace:
        trace.input = {"data": "important"}
        # Data is sent immediately when exiting the context

    # Deferred flush (default)
    with opik.start_as_current_trace("deferred-trace", flush=False) as trace:
        trace.input = {"data": "less urgent"}
        # Data will be sent asynchronously later or when the program exits
    ```
  </Tab>
</Tabs>

#### Best Practices

1. **Use descriptive names**: Choose clear, descriptive names for your traces and spans that explain what they represent.

2. **Set appropriate types**: U